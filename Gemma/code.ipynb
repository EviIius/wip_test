{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set the paths for the pretrained model and tokenizer\n",
    "MODEL_PATH = \"/path/to/your/pretrained/gemma\"  # Replace with the actual path to the model\n",
    "TOKENIZER_PATH = \"/path/to/your/tokenizer\"     # Replace with the actual path to the tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Move model to the appropriate device (CPU or GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to prompt the model\n",
    "def prompt_gemma(prompt_text, max_length=50):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Gemma model and retrieves the response.\n",
    "\n",
    "    Args:\n",
    "        prompt_text (str): The input text for the model.\n",
    "        max_length (int): Maximum length of the generated response.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt_text = \"Explain the importance of site preparation in construction.\"\n",
    "response = prompt_gemma(prompt_text)\n",
    "print(\"Gemma's Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
