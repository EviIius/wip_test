{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gemma(system_prompt, user_prompt, max_length=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Sends a system prompt and user prompt to the Gemma model and retrieves the response.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The context or system message for the model.\n",
    "        user_prompt (str): The input query from the user.\n",
    "        max_length (int): Maximum length of the generated response (default is 50 tokens).\n",
    "        temperature (float): Sampling temperature for randomness in generation (default is 0.7).\n",
    "        top_p (float): Top-p (nucleus) sampling for controlling token probabilities (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "    # Combine the system prompt and user prompt, clearly separating them\n",
    "    prompt_text = f\"System: {system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response with tunable parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the response and strip the input text\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    assistant_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "    return assistant_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt (context)\n",
    "system_prompt = (\n",
    "    \"You are an expert in summarizing technical documents. Your goal is to paraphrase input \"\n",
    "    \"texts for clarity and conciseness without losing technical details.\"\n",
    ")\n",
    "\n",
    "# Define the user's prompt\n",
    "user_prompt = \"Central Analytics Support & Enablement (CASE) is our enterprise support platform for over 20,000 data scientists, data analysts, MLOps engineers, BI developers, etc. at the bank. CASE includes multiple features such as 'CASEy' the chatbot, intent-driven (i.e., non-GAN/Auto-LLM) chatbot microservices built on MS Copilot Studio. Additionally, CASE includes the Collective Intelligence (CI) framework, conditional ticketing system, and its Graph-enabled search. All of this enables product and support teams to efficiently address customer needs, manage their product knowledge documentation, and handle product inquiries within a unified platform.\"\n",
    "\n",
    "# Call the prompt_gemma function\n",
    "response = prompt_gemma(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    max_length=150,  # Adjust length for longer responses\n",
    "    temperature=0.6, # Reduce randomness for concise responses\n",
    "    top_p=0.9        # Use top-p sampling\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(\"Gemma's Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace 'gpt2' with your preferred model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure the model and inputs are on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Explicitly set the padding token if it is not already defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set up the prompts\n",
    "def generate_response(system_prompt, user_prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response using the model with a system prompt and user prompt.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The system instruction to guide the conversation.\n",
    "        user_prompt (str): The user's input for the conversation.\n",
    "        max_length (int): The maximum length of the generated response.\n",
    "        temperature (float): Sampling temperature to control randomness.\n",
    "        top_p (float): Nucleus sampling top-p value.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "    # Combine system and user prompts\n",
    "    combined_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(combined_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    assistant_response = response.split(\"Assistant:\")[-1].strip()\n",
    "    return assistant_response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define system and user prompts\n",
    "    system_prompt = \"You are a helpful assistant specialized in construction-related topics.\"\n",
    "    user_prompt = \"Why is proper site preparation important in construction?\"\n",
    "\n",
    "    # Generate a response\n",
    "    response = generate_response(system_prompt, user_prompt, max_length=200, temperature=0.9, top_p=0.95)\n",
    "\n",
    "    print(\"Assistant Response:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
