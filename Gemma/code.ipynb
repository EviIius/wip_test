{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gemma(system_prompt, user_prompt, max_length=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Sends a system prompt and user prompt to the Gemma model and retrieves the response.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): The context or system message for the model.\n",
    "        user_prompt (str): The input query from the user.\n",
    "        max_length (int): Maximum length of the generated response (default is 50 tokens).\n",
    "        temperature (float): Sampling temperature for randomness in generation (default is 0.7).\n",
    "        top_p (float): Top-p (nucleus) sampling for controlling token probabilities (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "    # Combine the system prompt and user prompt, clearly separating them\n",
    "    prompt_text = f\"System: {system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response with tunable parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the response and strip the input text\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    assistant_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "    return assistant_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt (context)\n",
    "system_prompt = (\n",
    "    \"You are an expert in summarizing technical documents. Your goal is to paraphrase input \"\n",
    "    \"texts for clarity and conciseness without losing technical details.\"\n",
    ")\n",
    "\n",
    "# Define the user's prompt\n",
    "user_prompt = \"Central Analytics Support & Enablement (CASE) is our enterprise support platform for over 20,000 data scientists, data analysts, MLOps engineers, BI developers, etc. at the bank. CASE includes multiple features such as 'CASEy' the chatbot, intent-driven (i.e., non-GAN/Auto-LLM) chatbot microservices built on MS Copilot Studio. Additionally, CASE includes the Collective Intelligence (CI) framework, conditional ticketing system, and its Graph-enabled search. All of this enables product and support teams to efficiently address customer needs, manage their product knowledge documentation, and handle product inquiries within a unified platform.\"\n",
    "\n",
    "# Call the prompt_gemma function\n",
    "response = prompt_gemma(\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    max_length=150,  # Adjust length for longer responses\n",
    "    temperature=0.6, # Reduce randomness for concise responses\n",
    "    top_p=0.9        # Use top-p sampling\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(\"Gemma's Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "class GemmaChat:\n",
    "    def __init__(self, model_name=\"gemma-local-model-path\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        \"\"\"Sets the system prompt as a base context for the conversation.\"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def generate_response(self, user_prompt, max_length=200):\n",
    "        \"\"\"Generates a response using the system and user prompts.\"\"\"\n",
    "        # Combine the system prompt and user prompt\n",
    "        combined_prompt = f\"{self.system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(combined_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate output\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode and return the generated text\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract just the assistant's response (after \"Assistant:\")\n",
    "        assistant_response = response.split(\"Assistant:\")[-1].strip()\n",
    "        return assistant_response\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    gemma = GemmaChat(model_name=\"path_to_your_local_gemma_model\")\n",
    "\n",
    "    # Set system prompt\n",
    "    system_prompt = \"You are a helpful assistant skilled in answering technical and creative queries.\"\n",
    "    gemma.set_system_prompt(system_prompt)\n",
    "\n",
    "    # User prompt\n",
    "    user_prompt = \"Can you explain how transformers work in machine learning?\"\n",
    "\n",
    "    # Generate and print response\n",
    "    response = gemma.generate_response(user_prompt)\n",
    "    print(f\"Assistant: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
