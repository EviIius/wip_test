{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def create_llama_prompt(system_prompt: str, user_prompt: str, eos_token: str=\"<|endoftext|>\"):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt to use with LLaMA models that include system-level and user-level prompts,\n",
    "    with the specified EOS token for defining prompt boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    system_prompt (str): Instructions or context provided to the model as the system-level prompt.\n",
    "    user_prompt (str): The primary user query or command that requires a response.\n",
    "    eos_token (str): The end-of-sequence token to mark the end of different sections of the prompt.\n",
    "    \n",
    "    Returns:\n",
    "    str: A concatenated string that includes both system and user prompts, formatted with EOS tokens.\n",
    "    \"\"\"\n",
    "    prompt = f\"[SYSTEM]{eos_token} {system_prompt.strip()} {eos_token} [USER]{eos_token} {user_prompt.strip()} {eos_token}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_response(prompt: str, model_name: str=\"llama_model\", max_new_tokens: int=200):\n",
    "    \"\"\"\n",
    "    Generates a response using a LLaMA model with the given prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    prompt (str): The formatted prompt to pass to the LLaMA model.\n",
    "    model_name (str): The name or path of the pre-trained model to load.\n",
    "    max_new_tokens (int): The maximum number of tokens to generate for the response.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated response from the LLaMA model.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "    # Tokenize the prompt and generate tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the output tokens and extract the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()  # Remove the prompt from the response\n",
    "\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    system_prompt = \"You are an assistant who answers questions helpfully and politely.\"\n",
    "    user_prompt = \"What is the difference between machine learning and deep learning?\"\n",
    "    \n",
    "    # Create the formatted prompt\n",
    "    formatted_prompt = create_llama_prompt(system_prompt, user_prompt)\n",
    "    \n",
    "    # Generate a response using a LLaMA model\n",
    "    model_name = \"decapoda-research/llama-7b-hf\"  # Placeholder model path/name\n",
    "    response = generate_response(formatted_prompt, model_name)\n",
    "    \n",
    "    # Print the generated response\n",
    "    print(\"Response:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
