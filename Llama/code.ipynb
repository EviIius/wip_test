{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to answer the question:\n",
    "\n",
    "Identify the key information in the context.\n",
    "Use the information to answer the question directly.\n",
    "Provide evidence or reasoning from the context to support your answer.\n",
    "Input:\n",
    "Context: The water cycle describes how water moves through the environment. It involves processes like evaporation, condensation, precipitation, and collection. Evaporation occurs when water turns into vapor due to heat.\n",
    "Question: What is evaporation, and how does it occur?\n",
    "\n",
    "Output:\n",
    "\n",
    "Key Information: Evaporation is a process in the water cycle where water turns into vapor.\n",
    "Answer: Evaporation occurs when water turns into vapor due to heat.\n",
    "Evidence: This process is part of the water cycle, as stated in the context.\n",
    "Now, answer this question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAQC: \n",
    "\n",
    "General QAQC Review: Review the provided document for quality assurance. Identify any inconsistencies, factual inaccuracies, or areas requiring clarification. Suggest specific revisions to improve the document.\n",
    "\n",
    "QAQC for Document Compliance and Policy Adherence: Assess whether the document complies with internal policies, industry standards, or legal requirements. Identify any areas of non-compliance and suggest corrective measures.\n",
    "\n",
    "QAQC for Factual Accuracy and Completeness: Evaluate the text for factual accuracy and ensure all necessary details are included. Identify any gaps in information or potential errors, and propose solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Grammar:\n",
    "\n",
    "Professional Grammar Correction: Analyze the following text for grammatical accuracy, proper punctuation, and coherence. Provide a corrected version that adheres to formal English writing standards.\n",
    "\n",
    "Advanced Sentence Refinement: Review the given sentence for grammatical errors, stylistic inconsistencies, and clarity. Rewrite it to ensure it meets the standards of formal professional communication.\n",
    "\n",
    "Context-Specific Grammar and Style Improvement: Assess the provided text for grammatical accuracy and suitability for a formal professional setting. Provide an enhanced version with corrections and improved stylistic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q/A:\n",
    "\n",
    "Analytical Question/Answer Based on Text: Text: \"Regular exercise has been linked to improved mental health by reducing symptoms of anxiety and depression. Studies suggest that aerobic activities, such as running or swimming, are particularly effective.\"\n",
    "Q: \"Why might running or swimming be beneficial for mental health?\"\n",
    "A: \"Running or swimming might be beneficial for mental health because these aerobic activities have been shown to reduce symptoms of anxiety and depression.\"\n",
    "\n",
    "Inference-Based Question/Answer: Text: \"After weeks of negotiations, the two countries signed a historic peace treaty, ending decades of conflict and opening the door to economic cooperation.\"\n",
    "Q: \"What might be a potential outcome of the peace treaty?\"\n",
    "A: \"A potential outcome of the peace treaty could be increased economic cooperation between the two countries.\"\n",
    "\n",
    "Comparative Question/Answer: Text: \"City A has invested heavily in public transportation, resulting in reduced traffic congestion and lower carbon emissions. City B has focused on expanding highways, which has led to increased car usage and pollution.\"\n",
    "Q: \"How do the transportation strategies of City A and City B differ in their outcomes?\"\n",
    "A: \"City A's strategy of investing in public transportation has reduced traffic congestion and carbon emissions, while City B's focus on highway expansion has increased car usage and pollution.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def create_llama_prompt(system_prompt: str, user_prompt: str, eos_token: str=\"<|endoftext|>\"):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt to use with LLaMA models that include system-level and user-level prompts,\n",
    "    with the specified EOS token for defining prompt boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    system_prompt (str): Instructions or context provided to the model as the system-level prompt.\n",
    "    user_prompt (str): The primary user query or command that requires a response.\n",
    "    eos_token (str): The end-of-sequence token to mark the end of different sections of the prompt.\n",
    "    \n",
    "    Returns:\n",
    "    str: A concatenated string that includes both system and user prompts, formatted with EOS tokens.\n",
    "    \"\"\"\n",
    "    prompt = f\"[SYSTEM]{eos_token} {system_prompt.strip()} {eos_token} [USER]{eos_token} {user_prompt.strip()} {eos_token}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_response(prompt: str, model_name: str=\"llama_model\", max_new_tokens: int=200):\n",
    "    \"\"\"\n",
    "    Generates a response using a LLaMA model with the given prompt.\n",
    "    \n",
    "    Parameters:\n",
    "    prompt (str): The formatted prompt to pass to the LLaMA model.\n",
    "    model_name (str): The name or path of the pre-trained model to load.\n",
    "    max_new_tokens (int): The maximum number of tokens to generate for the response.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated response from the LLaMA model.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "    # Tokenize the prompt and generate tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the output tokens and extract the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()  # Remove the prompt from the response\n",
    "\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    system_prompt = \"You are an assistant who answers questions helpfully and politely.\"\n",
    "    user_prompt = \"What is the difference between machine learning and deep learning?\"\n",
    "    \n",
    "    # Create the formatted prompt\n",
    "    formatted_prompt = create_llama_prompt(system_prompt, user_prompt)\n",
    "    \n",
    "    # Generate a response using a LLaMA model\n",
    "    model_name = \"decapoda-research/llama-7b-hf\"  # Placeholder model path/name\n",
    "    response = generate_response(formatted_prompt, model_name)\n",
    "    \n",
    "    # Print the generated response\n",
    "    print(\"Response:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
