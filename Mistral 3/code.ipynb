{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Mistral 3 Prompt Template\n",
    "\n",
    "# Instructions:\n",
    "# - This prompt template is designed for use with the Mistral 3 model.\n",
    "# - It aims to provide a comprehensive guide to framing the prompt context.\n",
    "# - Use this template as a foundation to generate precise responses from the model.\n",
    "# - Replace placeholders with your desired values, and feel free to add specifics to refine the behavior you want.\n",
    "\n",
    "def mistral_3_prompt_template(context, task_description, input_data, format_specification, additional_instructions=None):\n",
    "    \"\"\"\n",
    "    Generates a complete prompt for the Mistral 3 model.\n",
    "\n",
    "    Parameters:\n",
    "    context (str): Background information or context for the task.\n",
    "    task_description (str): A clear description of what you need the model to do.\n",
    "    input_data (str): Any specific input or data the model should use.\n",
    "    format_specification (str): The required format for the model's output.\n",
    "    additional_instructions (str, optional): Any additional instructions to further specify the behavior of the response.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted prompt for the Mistral 3 model.\n",
    "    \"\"\"\n",
    "    # Base structure for the prompt\n",
    "    prompt = \"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Task:\n",
    "    {task_description}\n",
    "\n",
    "    Input:\n",
    "    {input_data}\n",
    "\n",
    "    Output Format:\n",
    "    {format_specification}\n",
    "    \"\"\".format(context=context, task_description=task_description, input_data=input_data, format_specification=format_specification)\n",
    "\n",
    "    # Add additional instructions if provided\n",
    "    if additional_instructions:\n",
    "        prompt += \"\\n\\nAdditional Instructions:\\n{additional_instructions}\".format(additional_instructions=additional_instructions)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Fill in the values according to your requirements\n",
    "    context = \"You are an expert assistant helping users understand natural language processing concepts.\"\n",
    "    task_description = \"Explain the concept of transformers in natural language processing in simple terms.\"\n",
    "    input_data = \"Transformers were introduced by Vaswani et al. in 2017.\"\n",
    "    format_specification = \"Provide the explanation in bullet points, each point no longer than two sentences.\"\n",
    "    additional_instructions = \"Ensure the explanation is suitable for someone with a beginner's understanding of machine learning.\"\n",
    "\n",
    "    # Generate the prompt\n",
    "    prompt = mistral_3_prompt_template(context, task_description, input_data, format_specification, additional_instructions)\n",
    "    print(prompt)\n",
    "\n",
    "    # Load model and tokenizer for local version of the LLM\n",
    "    model_name = \"mistral-3\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response with a shorter max_length to reduce response time\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=200, temperature=0.7, top_p=0.9)\n",
    "\n",
    "    # Decode and print the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
