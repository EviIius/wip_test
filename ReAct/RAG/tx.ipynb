{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee41354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prompt with a hardcoded ReAct framework\n",
    "prompt = f\"\"\"\n",
    "Model Metadata:\n",
    "{model_metadata}\n",
    "\n",
    "You are provided with detailed contents from the subsection of MDD titled \"{sub_section_name}\".\n",
    "This is the only context you may use. DO NOT incorporate any other information or your own prior knowledge.\n",
    "\n",
    "Follow the steps of the ReAct framework below to reason through and answer the question:\n",
    "\n",
    "1. Thought:\n",
    "   - Analyze the provided context carefully.\n",
    "   - Summarize the key points and details that are relevant.\n",
    "2. Action:\n",
    "   - Identify the specific information or process required by the question using only the above context.\n",
    "3. Observation:\n",
    "   - Reflect on your analysis and check that the context supports your conclusion.\n",
    "4. Final Answer:\n",
    "   - Based on your previous steps, provide a detailed, coherent, and complete answer to the following question.\n",
    "   \n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Combine with your base system prompt and process guidelines\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_individual_answers},\n",
    "    {\"role\": \"system\", \"content\": prompt}\n",
    "]\n",
    "outputs = []\n",
    "\n",
    "# Process each guideline by sending it as the user message,\n",
    "# generating the answer from the model, appending the result, and printing the output.\n",
    "%%time\n",
    "for guideline in guidelines:\n",
    "    # Here we set the current user query\n",
    "    messages = [{\"role\": \"user\", \"content\": guideline}]\n",
    "    # Generate model output using a function that applies the template\n",
    "    output = generate_chat_template(model=model, tokenizer=tokenizer, messages=messages, temperature=0.2, max_new_tokens=8000)\n",
    "    # Append the assistant's output so that context accumulates if needed\n",
    "    messages += [{\"role\": \"assistant\", \"content\": output}]\n",
    "    print(guideline)\n",
    "    print(\"\".join([\"#\"]*15))\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")\n",
    "    outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d298bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "%%time\n",
    "for guideline in guidelines:\n",
    "    # Use the current guideline as the query for retrieval.\n",
    "    search_results = vector_store.search(guideline, k=3)\n",
    "    \n",
    "    # Concatenate retrieved document chunks to form the metadata context.\n",
    "    retrieved_context = \"\"\n",
    "    for doc, score in search_results:\n",
    "        retrieved_context += f\"\\n---\\n{doc.content}\\n\"\n",
    "    \n",
    "    # Build a prompt incorporating the chain-of-thought instructions and the retrieved metadata context.\n",
    "    prompt = f\"\"\"\n",
    "Model Metadata:\n",
    "{model_metadata}\n",
    "\n",
    "You are provided with chunked metadata context below extracted from our documents.\n",
    "Do NOT incorporate any external knowledge or your internal knowledge beyond this context.\n",
    "\n",
    "Retrieved Metadata Context:\n",
    "{retrieved_context}\n",
    "\n",
    "Follow these steps:\n",
    "1. Thought: Analyze the metadata context and the question.\n",
    "2. Action: Reason using the provided context.\n",
    "3. Observation: Reflect on the key points from the retrieved metadata.\n",
    "4. Final Answer: Produce a detailed, coherent, and complete answer to the question.\n",
    "\n",
    "Question: {guideline}\n",
    "\"\"\"\n",
    "    \n",
    "    # Set up the messages for generate_chat_template.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_individual_answers},\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "    # Append the guideline again as a user message.\n",
    "    messages.append({\"role\": \"user\", \"content\": guideline})\n",
    "    \n",
    "    # Generate the answer using the chat template function.\n",
    "    output = generate_chat_template(model=model, tokenizer=tokenizer, messages=messages, temperature=0.2, max_new_tokens=8000)\n",
    "    \n",
    "    # Accumulate the assistant's output into the messages for potential context chaining.\n",
    "    messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "    \n",
    "    # Print and store the result.\n",
    "    print(guideline)\n",
    "    print(\"\".join([\"#\"*15]))\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")\n",
    "    outputs.append(output)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
