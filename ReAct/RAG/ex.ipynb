{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant doc chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a single combined context from the top docs\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, generate a snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Use only the provided context if needed.\n",
    "\"\"\"\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_chain(user_query: str, combined_context: str, guideline: str, idx: int,\n",
    "                    max_new_tokens: int = 1024, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a chain-of-thought prompt using the ReAct framework for a single guideline,\n",
    "    and uses the LLaMA get_completion function (from LatestHelp.py) to generate a response.\n",
    "    \n",
    "    Parameters:\n",
    "      user_query (str): The model validation question.\n",
    "      combined_context (str): The extracted context from MDD subsections containing model information.\n",
    "      guideline (str): The specific guideline instruction.\n",
    "      idx (int): The guideline index number.\n",
    "      max_new_tokens (int): Maximum token length for the LLaMA model output.\n",
    "      temperature (float): Sampling temperature for the output generation.\n",
    "    \n",
    "    Returns:\n",
    "      str: The response generated by the LLaMA model.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Using this RAG function, I want you to implement a hardcoded version of the ReAct framework to do the following:\n",
    "Your task is to assist a Quantitative Model Validator working in the Model Risk Management team of a bank, to find answers to policy questions about Model Development Document (MDD) based on the provided context.\n",
    "Contents of the subsection(s) of MDD is used as the only input context to answer the Model Validation policy questions. You are a highly accurate assistant who strictly answers only based on the information in the provided context.\n",
    "\n",
    "Strictly follow these Generation Instructions:\n",
    "- Your response should be accurate, coherent, detailed, and descriptive by including all the important statistics, tables, terminologies, and definitions.\n",
    "- Your response should be relevant to the question being asked.\n",
    "- Your response should be honest, focused, and grounded in the provided context.\n",
    "- Do not change or assume any definition, terminology, statistical data, numerical information, or table information.\n",
    "- Always respond with \"Not found\" when you cannot find relevant information in the context.\n",
    "- Always respond with \"Not found\" if any information asked is not explicitly mentioned.\n",
    "- Use the important keywords and phrases from the context to frame your response.\n",
    "- Use bullet points only when required.\n",
    "- Only use the information provided under the specific product, business segment or aspect when answering questions. If the context includes details about multiple products, ensure your response is limited to the product specified in the query. Do not include information from other products, businesses, or other aspects.\n",
    "\n",
    "### Thought\n",
    "[Your initial analysis of the problem]\n",
    "### Action: Step 1\n",
    "[First concrete action to take/solve]\n",
    "### Observation\n",
    "[What are the results from this action?]\n",
    "### Action: Step 2\n",
    "[Next action based on previous observation]\n",
    "### Observation\n",
    "[Results from second action]\n",
    "\n",
    "...(repeat actions/observations as needed)\n",
    "\n",
    "### Final Answer\n",
    "[Your conclusive answer based on the chain of reasoning]\n",
    "\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise section of a final document that follows this guideline.\n",
    "Focus on using the available context effectively.\n",
    "\"\"\"\n",
    "    # Call the LLaMA model's get_completion function (from LatestHelp.py) to generate the answer.\n",
    "    return get_completion(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "\n",
    "def run_react_chain_loop(user_query: str, combined_context: str, guidelines: list,\n",
    "                         max_new_tokens: int = 1024, temperature: float = 0.7) -> dict:\n",
    "    \"\"\"\n",
    "    Iterates over a list of guidelines, calling the run_react_chain function for each one.\n",
    "    Returns a dictionary mapping guideline indices to their respective generated responses.\n",
    "    \n",
    "    Parameters:\n",
    "      user_query (str): The model validation question.\n",
    "      combined_context (str): The extracted context containing model information.\n",
    "      guidelines (list): A list of guideline strings that each describe a section of the final document.\n",
    "      max_new_tokens (int): Maximum token length for each LLaMA model output.\n",
    "      temperature (float): Sampling temperature for the output generation.\n",
    "    \n",
    "    Returns:\n",
    "      dict: A dictionary where each key is the guideline index and the value is the generated answer.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        answer = run_react_chain(user_query, combined_context, guideline, idx,\n",
    "                                 max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        results[idx] = answer\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # The user query and combined_context should be replaced with actual model validation details and context.\n",
    "    sample_query = \"Describe the growth of the portfolio over the past 5, 10 years.\"\n",
    "    sample_context = (\n",
    "        \"Section X: The portfolio has shown consistent growth in balance sheet contributions \"\n",
    "        \"over the past 10 years with a 5% annual increase. Additional details about products \"\n",
    "        \"and market conditions are stored in subsequent sections.\"\n",
    "    )\n",
    "    \n",
    "    guidelines = [\n",
    "        \"Briefly describe the business portfolio to which the model applies\",\n",
    "        \"Include the products and business segments offered by the business line\",\n",
    "        \"Describe the products of the LOB and portfolio to which this model applies.\",\n",
    "        \"Describe any current or planned changes in the products, channels, policies, programs, organization, or marketing practices that may impact the model under consideration.\",\n",
    "        \"Assess how close the current customer base to the target customer profile is.\",\n",
    "        \"Consider whether the customer base is likely to shift over the lifetime of the model.\",\n",
    "        \"Specify the current and possible future market conditions and the impact they may have on the portfolio and the model.\",\n",
    "        \"Describe the growth of the portfolio over the past 5, 10, X years, both in size and in significance to the balance sheet.\",\n",
    "        \"If this is a revalidation or the model replaces an existing model, highlight the key relevant changes on the business between the previous model developments and model validations, and this validation.\",\n",
    "        \"Include all the changes related to modeling, such as model framework and theory, variables, data sources, and programs; \"\n",
    "        \"business changes, such as policy or strategy; environmental changes, such as competitor actions, economic changes, and political or regulatory changes; \"\n",
    "        \"and any other changes that impact the model, its implementation, evaluation, and usage.\",\n",
    "        \"Include a table or summary of the portfolio, product, or business metrics of the business in the most recent and past periods. \"\n",
    "        \"These can include metrics such as balances, losses, recoveries, number of accounts, average account size, credit limits, etc.\"\n",
    "    ]\n",
    "    \n",
    "    outputs = run_react_chain_loop(sample_query, sample_context, guidelines)\n",
    "    for idx, answer in outputs.items():\n",
    "        print(f\"Guideline #{idx} Answer:\")\n",
    "        print(answer)\n",
    "        print(\"----------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
