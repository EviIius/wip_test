{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_guidelines(\n",
    "    query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Executes a search against your existing vector store, then loops over each\n",
    "    guideline to generate a separate response snippet. Returns one combined output.\n",
    "\n",
    "    Parameters:\n",
    "      - query (str): The user's original question or prompt.\n",
    "      - guidelines (List[str]): A list of guidelines to loop through.\n",
    "      - vector_store (VectorStore): Your existing VectorStore with a .search(...) method.\n",
    "      - k (int): Number of top matching chunks to retrieve from the vector store.\n",
    "      - max_new_tokens (int): Max tokens for each LLM response.\n",
    "      - temperature (float): Sampling temperature for the LLM.\n",
    "\n",
    "    Returns:\n",
    "      - A single string that combines each response snippet under its respective guideline.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Search your vector store for top-k relevant chunks:\n",
    "    search_results = vector_store.search(query, k=k)\n",
    "    \n",
    "    # 2) Optionally, assemble the top chunks into a single context string:\n",
    "    combined_context = \"\\n\\n\".join([doc.content for doc, _ in search_results])\n",
    "\n",
    "    # 3) Loop over each guideline and generate a snippet for each:\n",
    "    final_output = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        # Build a prompt that includes:\n",
    "        #   - The original user query\n",
    "        #   - The combined context from your top k search results\n",
    "        #   - The specific guideline to follow\n",
    "        prompt = f\"\"\"\n",
    "You are given the following user query:\n",
    "{query}\n",
    "\n",
    "Here is some relevant context from our documents:\n",
    "{combined_context}\n",
    "\n",
    "Now apply the following guideline:\n",
    "{guideline}\n",
    "\n",
    "Please generate a concise response that follows this guideline.\n",
    "\"\"\"\n",
    "\n",
    "        # 4) Use your LLM completion function to get the snippet:\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # 5) Format this snippet and store it:\n",
    "        formatted_snippet = (\n",
    "            f\"### Guideline {idx}: {guideline}\\n\\n\"\n",
    "            f\"**Response**:\\n{snippet.strip()}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        final_output.append(formatted_snippet)\n",
    "\n",
    "    # 6) Combine all guideline-based responses into one big string:\n",
    "    return \"\\n\".join(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_guidelines = [\n",
    "    \"Use formal language.\",\n",
    "    \"Focus on the financial aspects.\",\n",
    "    \"Provide a step-by-step plan.\"\n",
    "]\n",
    "\n",
    "response_text = query_with_guidelines(\n",
    "    query=\"How should I allocate my budget?\",\n",
    "    guidelines=my_guidelines,\n",
    "    vector_store=existing_vector_store,\n",
    "    k=3,               # top-3 relevant chunks\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
