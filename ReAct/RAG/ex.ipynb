{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONFIG: Adjust as needed\n",
    "# ------------------------------------------------------------------\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "# Force pad_token to eos if missing\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load models (in eval mode, no gradients)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "embedding_model.requires_grad_(False)\n",
    "\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "completion_model.requires_grad_(False)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = re.sub(r\"\\.{2,}\", \"\", text)\n",
    "        text = re.sub(r\"\\s*\\u2002\\s*\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        lines = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if not line or all(c in \".-\" for c in line):\n",
    "                continue\n",
    "            if line.startswith(\"=== Page\"):\n",
    "                lines.append(line)\n",
    "                continue\n",
    "            if re.match(r\"^\\d+[-â€“]\\d+$\", line):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        from PyPDF2 import PdfReader\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page_num, page in enumerate(reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text += f\"\\n=== Page {page_num} ===\\n{page_text}\"\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_txt(file_path: str) -> str:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_path: str) -> str:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        return DocumentProcessor.clean_text(df.to_string(index=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text(\n",
    "        text: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ) -> List[\"Document\"]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        end = chunk_size\n",
    "        while start < len(text):\n",
    "            chunk = text[start:end]\n",
    "            if not chunk.strip():\n",
    "                break\n",
    "            chunks.append(Document(content=chunk.strip()))\n",
    "\n",
    "            # Overlap\n",
    "            start = end - chunk_overlap\n",
    "            end = start + chunk_size\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_chunk_file(file_path: str) -> List[\"Document\"]:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = DocumentProcessor.load_pdf(file_path)\n",
    "        elif ext == \".txt\":\n",
    "            text = DocumentProcessor.load_txt(file_path)\n",
    "        elif ext == \".csv\":\n",
    "            text = DocumentProcessor.load_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "        return DocumentProcessor.chunk_text(text)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# EMBEDDING & COMPLETION\n",
    "# ------------------------------------------------------------------\n",
    "def get_embedding(\n",
    "    text: str,\n",
    "    max_length: int = 256\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Create a single text embedding using the last hidden state of LLaMA, \n",
    "    normalized to unit length. We TRUNCATE to max_length to avoid OOM.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        emb = hidden_states.mean(dim=1).squeeze()\n",
    "        emb = emb / emb.norm(p=2)\n",
    "    return emb.cpu().numpy().tolist()\n",
    "\n",
    "def get_embeddings_batch(\n",
    "    texts: List[str],\n",
    "    batch_size: int = 2,   # Use small batch size to avoid OOM\n",
    "    max_length: int = 256\n",
    ") -> List[List[float]]:\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            for idx in range(len(batch)):\n",
    "                emb = hidden_states[idx].mean(dim=0)\n",
    "                emb = emb / emb.norm(p=2)\n",
    "                embeddings.append(emb.cpu().numpy().tolist())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# VECTOR STORE (FAISS)\n",
    "# ------------------------------------------------------------------\n",
    "class VectorStore:\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        index_path = self._get_index_path()\n",
    "        docs_path = self._get_documents_path()\n",
    "        if os.path.exists(index_path) and os.path.exists(docs_path):\n",
    "            try:\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(docs_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded index with {len(self.documents)} documents.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Error loading index:\", e)\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error saving index:\", e)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating a new FAISS index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "\n",
    "        # Get embeddings in small batches, truncated\n",
    "        embeddings = get_embeddings_batch(contents, batch_size=2, max_length=256)\n",
    "\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "\n",
    "        # Single-embedding with truncation\n",
    "        query_emb = get_embedding(query, max_length=256)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_emb]).astype(\"float32\"),\n",
    "            k\n",
    "        )\n",
    "        similarities = 1 / (1 + distances)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            sim_score = similarities[0][i]\n",
    "            results.append((doc, sim_score))\n",
    "        return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# HYBRID SEARCHER\n",
    "# ------------------------------------------------------------------\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Example hybrid approach combining a TF-IDF search with a VectorStore search.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.vector_store = VectorStore(persist_directory)\n",
    "\n",
    "    def create_index(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "        self._initialize_tfidf()\n",
    "        self.vector_store.create_index(documents, force_recreate=True)\n",
    "\n",
    "    def _initialize_tfidf(self):\n",
    "        contents = [doc.content for doc in self.documents]\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(contents)\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        # Vector store\n",
    "        vector_results = self.vector_store.search(query, k)\n",
    "\n",
    "        # TF-IDF\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        keyword_indices = np.argsort(keyword_scores)[-k:][::-1]\n",
    "        keyword_results = [(self.documents[i], keyword_scores[i]) for i in keyword_indices]\n",
    "\n",
    "        # Combine\n",
    "        seen = set()\n",
    "        combined = []\n",
    "        for doc, score in (vector_results + keyword_results):\n",
    "            if doc.content not in seen:\n",
    "                seen.add(doc.content)\n",
    "                combined.append((doc, score))\n",
    "\n",
    "        # Sort by descending score\n",
    "        return sorted(combined, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_strings = [f\"{k}: {v}\" for k,v in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_strings)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    final_segments = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline.\n",
    "Use only the provided context as needed.\n",
    "\"\"\"\n",
    "        segment_response = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_segment = f\"### GUIDELINE #{idx}: {guideline}\\n{segment_response.strip()}\\n\"\n",
    "        final_segments.append(formatted_segment)\n",
    "\n",
    "    final_document = \"\\n\\n\".join(final_segments)\n",
    "    return final_document\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# USAGE EXAMPLE\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load & chunk a sample text (adjust file_path as needed)\n",
    "    file_path = \"sample.txt\"  # or sample.pdf, etc.\n",
    "    doc_chunks = DocumentProcessor.load_and_chunk_file(file_path)\n",
    "\n",
    "    # 2) Create a VectorStore (or HybridSearcher) & index these chunks\n",
    "    vs = VectorStore(persist_directory=\"rag_index\")\n",
    "    vs.create_index(doc_chunks, force_recreate=True)  # Rebuild index\n",
    "\n",
    "    # 3) Example guidelines\n",
    "    my_guidelines = [\n",
    "        \"Provide a step-by-step approach.\",\n",
    "        \"Use non-technical language as much as possible.\"\n",
    "    ]\n",
    "\n",
    "    # 4) Generate document\n",
    "    user_query = \"How do I organize my personal finances effectively?\"\n",
    "    final_doc = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=my_guidelines,\n",
    "        vector_store=vs,\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== FINAL DOCUMENT =====\\n\")\n",
    "    print(final_doc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
