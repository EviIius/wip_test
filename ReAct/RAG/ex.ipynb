{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIG: Modify to suit your environment/model\n",
    "# ------------------------------------------------------------------------------\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "# Ensure pad_token is set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model for embeddings and completion (no 8-bit quantization, just float16 if GPU)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Simple wrapper for chunk text and any associated metadata.\n",
    "    \"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT PROCESSOR\n",
    "# ------------------------------------------------------------------------------\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Helper to load PDF, TXT, CSV files, then chunk them into Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = re.sub(r\"\\.{2,}\", \"\", text)\n",
    "        text = re.sub(r\"\\s*\\u2002\\s*\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        lines = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if not line or all(c in \".-\" for c in line):\n",
    "                continue\n",
    "            if line.startswith(\"=== Page\"):\n",
    "                lines.append(line)\n",
    "                continue\n",
    "            if re.match(r\"^\\d+[-–]\\d+$\", line):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        from PyPDF2 import PdfReader\n",
    "\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page_num, page in enumerate(reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text += f\"\\n=== Page {page_num} ===\\n{page_text}\"\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_txt(file_path: str) -> str:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_path: str) -> str:\n",
    "        df = pd.read_csv(file_path, dtype=str)  # read all columns as string\n",
    "        return DocumentProcessor.clean_text(df.to_string(index=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Very basic chunking approach—splits text into blocks of 'chunk_size' \n",
    "        with 'chunk_overlap' overlap between consecutive chunks.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        end = chunk_size\n",
    "        while start < len(text):\n",
    "            chunk = text[start:end]\n",
    "            if not chunk.strip():\n",
    "                break\n",
    "            doc = Document(content=chunk.strip())\n",
    "            chunks.append(doc)\n",
    "\n",
    "            # Overlap region\n",
    "            start = end - chunk_overlap\n",
    "            end = start + chunk_size\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_chunk_file(file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load a PDF/TXT/CSV, clean it, and chunk into Document objects.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = DocumentProcessor.load_pdf(file_path)\n",
    "        elif ext == \".txt\":\n",
    "            text = DocumentProcessor.load_txt(file_path)\n",
    "        elif ext == \".csv\":\n",
    "            text = DocumentProcessor.load_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "        return DocumentProcessor.chunk_text(text)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EMBEDDING & COMPLETION HELPERS\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Creates a single text embedding using the last hidden state \n",
    "    of LLaMA, normalized to unit length.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # last hidden layer\n",
    "        embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        embedding = embedding / embedding.norm(p=2)\n",
    "    return embedding.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Get embeddings in batches, each text is embedded by calling get_embedding.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = [get_embedding(text) for text in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the LLaMA model with basic sampling.\n",
    "    \"\"\"\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,  # guard input length\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# VECTOR STORE (FAISS) - Original Logic\n",
    "# ------------------------------------------------------------------------------\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Stores embeddings of Documents in a FAISS index and allows for\n",
    "    similarity search. This uses the original 'create_index' logic \n",
    "    that was working for you before.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        index_path = self._get_index_path()\n",
    "        docs_path = self._get_documents_path()\n",
    "        if os.path.exists(index_path) and os.path.exists(docs_path):\n",
    "            try:\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(docs_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded existing index with {len(self.documents)} documents.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Error loading index:\", e)\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error saving index:\", e)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating new index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "\n",
    "        # Original approach: get all embeddings at once (in batches, default=32)\n",
    "        embeddings = get_embeddings_batch(contents)\n",
    "\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Searches for similar documents. L2 distance is converted to similarity=1/(1+distance).\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "\n",
    "        query_embedding = get_embedding(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding]).astype(\"float32\"), k\n",
    "        )\n",
    "        similarities = 1 / (1 + distances)\n",
    "\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            sim_score = similarities[0][i]\n",
    "            results.append((doc, sim_score))\n",
    "        return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# HYBRID SEARCHER (Optional)\n",
    "# ------------------------------------------------------------------------------\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Example hybrid approach combining a TF-IDF search with a VectorStore search.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.vector_store = VectorStore(persist_directory)\n",
    "\n",
    "    def create_index(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "        self._initialize_tfidf()\n",
    "        self.vector_store.create_index(documents, force_recreate=True)\n",
    "\n",
    "    def _initialize_tfidf(self):\n",
    "        contents = [doc.content for doc in self.documents]\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(contents)\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        # Vector-based results\n",
    "        vector_results = self.vector_store.search(query, k)\n",
    "\n",
    "        # TF-IDF results\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        keyword_indices = np.argsort(keyword_scores)[-k:][::-1]\n",
    "        keyword_results = [(self.documents[i], keyword_scores[i]) for i in keyword_indices]\n",
    "\n",
    "        # Combine\n",
    "        seen = set()\n",
    "        combined_results = []\n",
    "        for doc, score in (vector_results + keyword_results):\n",
    "            if doc.content not in seen:\n",
    "                seen.add(doc.content)\n",
    "                combined_results.append((doc, score))\n",
    "\n",
    "        # Sort by final score desc\n",
    "        return sorted(combined_results, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant doc chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a single combined context from the top docs\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, generate a snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Use only the provided context if needed.\n",
    "\"\"\"\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXAMPLE USAGE (Comment out if you only want library code)\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Suppose we load & chunk a sample file (PDF, TXT, or CSV)\n",
    "    file_path = \"sample.txt\"  # adjust to your actual file\n",
    "    doc_chunks = DocumentProcessor.load_and_chunk_file(file_path)\n",
    "\n",
    "    # 2) Create a vector store and index the doc chunks\n",
    "    vs = VectorStore(persist_directory=\"rag_index\")\n",
    "    vs.create_index(doc_chunks, force_recreate=True)\n",
    "\n",
    "    # 3) Some example guidelines\n",
    "    my_guidelines = [\n",
    "        \"Provide a step-by-step approach.\",\n",
    "        \"Use plain language for a broad audience.\"\n",
    "    ]\n",
    "\n",
    "    # 4) Generate a combined doc addressing each guideline\n",
    "    user_query = \"How can I plan my personal finances effectively?\"\n",
    "    final_doc = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=my_guidelines,\n",
    "        vector_store=vs,\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== FINAL DOCUMENT =====\\n\")\n",
    "    print(final_doc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
