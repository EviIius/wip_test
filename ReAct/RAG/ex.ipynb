{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a file (PDF, TXT, or CSV), clean the text, and chunk it into Document objects.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        text = DocumentProcessor.load_pdf(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        text = DocumentProcessor.load_txt(file_path)\n",
    "    elif ext == \".csv\":\n",
    "        text = DocumentProcessor.load_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "    # Chunk the text\n",
    "    documents = DocumentProcessor.chunk_text(text)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_documents(file_path):\n",
    "    \"\"\"\n",
    "    Load a file (PDF, TXT, or CSV) into Documents, then create a new VectorStore index.\n",
    "    \"\"\"\n",
    "    # Load and chunk document\n",
    "    documents = load_file(file_path)\n",
    "\n",
    "    # Create and return vector store\n",
    "    vector_store = VectorStore()  # or HybridSearcher() if you want both vector + keyword\n",
    "    vector_store.create_index(documents, force_recreate=True)\n",
    "    print(f\"Processed {len(documents)} chunks from file: {file_path}\")\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = load_documents(\"/commons/corpra_share/k152356/ReAct_Testing/metatext.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant doc chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a single combined context from the top docs\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, generate a snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Use only the provided context if needed.\n",
    "\"\"\"\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: list,\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant document chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \n",
    "    Parameters:\n",
    "      user_query (str): The model validation question.\n",
    "      guidelines (list): A list of guideline strings that define sections of the final document.\n",
    "      vector_store (VectorStore): An instance of the VectorStore used for retrieving context.\n",
    "      k (int): The number of top documents to retrieve from the vector store.\n",
    "      max_new_tokens (int): Maximum tokens for each generated snippet.\n",
    "      temperature (float): Sampling temperature for generation.\n",
    "    \n",
    "    Returns:\n",
    "      str: The final combined document based on the generated guideline sections.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a combined context from the top documents\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Iterate over each guideline and generate the associated snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "Using this RAG function, I want you to implement a hardcoded version of the ReAct framework to do the following:\n",
    "Your task is to assist a Quantitative Model Validator working in the Model Risk Management team of a bank, to find answers to policy questions about Model Development Document (MDD) based on the provided context.\n",
    "Contents of the subsection(s) of MDD is used as the only input context to answer the Model Validation policy questions. You are a highly accurate assistant who strictly answers only based on the information in the provided context.\n",
    "\n",
    "Strictly follow these Generation Instructions:\n",
    "- Your response should be accurate, coherent, detailed, and descriptive by including all the important statistics, tables, terminologies, and definitions.\n",
    "- Your response should be relevant to the question being asked.\n",
    "- Your response should be honest, focused, and grounded in the provided context.\n",
    "- Do not change or assume any definition, terminology, statistical data, numerical information, or table information.\n",
    "- Always respond with \"Not found\" when you cannot find relevant information in the context.\n",
    "- Always respond with \"Not found\" if any information asked is not explicitly mentioned.\n",
    "- Use the important keywords and phrases from the context to frame your response.\n",
    "- Use bullet points only when required.\n",
    "- Only use the information provided under the specific product, business segment or aspect when answering questions. If the context includes details about multiple products, ensure your response is limited to the product specified in the query. Do not include information from other products, businesses, or other aspects.\n",
    "\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise section of a final document that follows this guideline.\n",
    "Focus on using the available context effectively.\n",
    "\"\"\"\n",
    "        # Generate snippet using the LLaMA model's get_completion function\n",
    "        snippet = get_completion(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets into one document\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define a sample user query and a list of guidelines for generating the document sections.\n",
    "    user_query = \"Describe the growth of the portfolio over the past 5, 10 years.\"\n",
    "    guidelines = [\n",
    "        \"Briefly describe the business portfolio to which the model applies\",\n",
    "        \"Include the products and business segments offered by the business line\",\n",
    "        \"Describe the products of the LOB and portfolio to which this model applies.\",\n",
    "        \"Describe any current or planned changes in the products, channels, policies, programs, organization, or marketing practices that may impact the model under consideration.\",\n",
    "        \"Assess how close the current customer base to the target customer profile is.\",\n",
    "        \"Consider whether the customer base is likely to shift over the lifetime of the model.\",\n",
    "        \"Specify the current and possible future market conditions and the impact they may have on the portfolio and the model.\",\n",
    "        \"Describe the growth of the portfolio over the past 5, 10, X years, both in size and in significance to the balance sheet.\",\n",
    "        \"If this is a revalidation or the model replaces an existing model, highlight the key relevant changes on the business between the previous model developments and model validations, and this validation.\",\n",
    "        \"Include all the changes related to modeling, such as model framework and theory, variables, data sources, and programs; business changes, such as policy or strategy; environmental changes, such as competitor actions, economic changes, and political or regulatory changes; and any other changes that impact the model, its implementation, evaluation, and usage.\",\n",
    "        \"Include a table or summary of the portfolio, product, or business metrics of the business in the most recent and past periods. These can include metrics such as balances, losses, recoveries, number of accounts, average account size, credit limits, etc.\"\n",
    "    ]\n",
    "\n",
    "    # Generate the final document by integrating the context with guideline responses.\n",
    "    final_document = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=guidelines,\n",
    "        vector_store=vector_store,  # Assuming vector_store is defined and initialized\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(\"Final Document:\")\n",
    "    print(final_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# all_in_one.py\n",
    "# =============================================================================\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIG: Modify to suit your environment/model\n",
    "# ------------------------------------------------------------------------------\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "# Ensure pad_token is set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model for embeddings (no quantization, float16 if GPU)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# Load Model for completion\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES AND HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Simple wrapper for chunk text and any associated metadata.\n",
    "    \"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Helper to load PDF, TXT, CSV files, then chunk them into Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans up extra whitespace, weird chars, repeated periods, etc.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"\\.{2,}\", \"\", text)\n",
    "        text = re.sub(r\"\\s*\\u2002\\s*\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        lines = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            # Skip empty lines or lines of only punctuation\n",
    "            if not line or all(c in \".-\" for c in line):\n",
    "                continue\n",
    "            # Keep page markers if you want them\n",
    "            if line.startswith(\"=== Page\"):\n",
    "                lines.append(line)\n",
    "                continue\n",
    "            # Skip page ranges\n",
    "            if re.match(r\"^\\d+[-–]\\d+$\", line):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses PyPDF2 to extract text from a PDF file.\n",
    "        \"\"\"\n",
    "        from PyPDF2 import PdfReader\n",
    "\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page_num, page in enumerate(reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text += f\"\\n=== Page {page_num} ===\\n{page_text}\"\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_txt(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Loads a .txt file directly, then cleans the text.\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Loads a CSV file into a pandas DataFrame, then \n",
    "        dumps the DataFrame to a string and cleans it.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path, dtype=str)  # read all columns as string\n",
    "        return DocumentProcessor.clean_text(df.to_string(index=False))\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Very basic chunking approach—splits text into blocks of 'chunk_size' \n",
    "        with 'chunk_overlap' overlap between consecutive chunks.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        end = chunk_size\n",
    "        while start < len(text):\n",
    "            chunk = text[start:end]\n",
    "            if not chunk.strip():\n",
    "                break\n",
    "            doc = Document(content=chunk.strip())\n",
    "            chunks.append(doc)\n",
    "\n",
    "            # Move by chunk_size but overlap chunk_overlap\n",
    "            start = end - chunk_overlap\n",
    "            end = start + chunk_size\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_chunk_file(file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load a PDF/TXT/CSV, clean it, and chunk into Document objects.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = DocumentProcessor.load_pdf(file_path)\n",
    "        elif ext == \".txt\":\n",
    "            text = DocumentProcessor.load_txt(file_path)\n",
    "        elif ext == \".csv\":\n",
    "            text = DocumentProcessor.load_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "        return DocumentProcessor.chunk_text(text)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EMBEDDING & COMPLETION HELPERS\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Creates a single text embedding using the last hidden state \n",
    "    of the embedding_model, normalized to unit length.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # last hidden layer\n",
    "        embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        embedding = embedding / embedding.norm(p=2)\n",
    "    return embedding.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Get embeddings in batches by calling get_embedding for each text.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = [get_embedding(text) for text in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the LLaMA-based model with sampling.\n",
    "    \"\"\"\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,  # guard input length\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# VECTOR STORE (FAISS)\n",
    "# ------------------------------------------------------------------------------\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Stores embeddings of Documents in a FAISS index and allows for\n",
    "    similarity search. This uses the 'create_index' logic to handle\n",
    "    indexing and retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Attempt to load a local FAISS index and the associated documents \n",
    "        from the persist_directory, if they exist.\n",
    "        \"\"\"\n",
    "        index_path = self._get_index_path()\n",
    "        docs_path = self._get_documents_path()\n",
    "        if os.path.exists(index_path) and os.path.exists(docs_path):\n",
    "            try:\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(docs_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded existing index with {len(self.documents)} documents.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Error loading index:\", e)\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        \"\"\"\n",
    "        Save the FAISS index and the associated documents to the local\n",
    "        persist_directory.\n",
    "        \"\"\"\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error saving index:\", e)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        \"\"\"\n",
    "        Creates a new FAISS index from the provided documents, unless \n",
    "        an existing index is already loaded (and force_recreate is False).\n",
    "        \"\"\"\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating new index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "\n",
    "        # Get embeddings for each chunk\n",
    "        embeddings = get_embeddings_batch(contents)\n",
    "\n",
    "        # Initialize a FAISS index\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Searches for similar documents via L2 distance. \n",
    "        We convert L2 distance to similarity = 1 / (1 + distance).\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "\n",
    "        query_embedding = get_embedding(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding]).astype(\"float32\"), k\n",
    "        )\n",
    "        similarities = 1 / (1 + distances)\n",
    "\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            sim_score = similarities[0][i]\n",
    "            results.append((doc, sim_score))\n",
    "        return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# HYBRID SEARCHER (Vector + TF-IDF)\n",
    "# ------------------------------------------------------------------------------\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Combines a TF-IDF approach with a VectorStore approach, returning \n",
    "    top-k documents based on combined result sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.vector_store = VectorStore(persist_directory)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = True):\n",
    "        \"\"\"\n",
    "        Create a fresh TF-IDF matrix and FAISS index with the given documents.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self._initialize_tfidf()\n",
    "        self.vector_store.create_index(documents, force_recreate=force_recreate)\n",
    "\n",
    "    def _initialize_tfidf(self):\n",
    "        contents = [doc.content for doc in self.documents]\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(contents)\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        # Vector-based results\n",
    "        vector_results = self.vector_store.search(query, k)\n",
    "\n",
    "        # TF-IDF results\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        keyword_indices = np.argsort(keyword_scores)[-k:][::-1]\n",
    "        keyword_results = [(self.documents[i], keyword_scores[i]) for i in keyword_indices]\n",
    "\n",
    "        # Combine results, ensuring no duplicates\n",
    "        seen = set()\n",
    "        combined_results = []\n",
    "        for doc, score in (vector_results + keyword_results):\n",
    "            if doc.content not in seen:\n",
    "                seen.add(doc.content)\n",
    "                combined_results.append((doc, score))\n",
    "\n",
    "        # Sort again by descending score\n",
    "        combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return combined_results[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from LatestHelp import (\n",
    "    load_documents,\n",
    "    get_completion,  # This is used for generating text completions.\n",
    "    Document,        # The Document data structure.\n",
    ")\n",
    "\n",
    "# Dummy implementation for generate_chat_template.\n",
    "# Replace this with your actual generation function as required.\n",
    "def generate_chat_template(model, tokenizer, messages, temperature, max_new_tokens):\n",
    "    # For demonstration, this dummy function combines all messages and calls get_completion.\n",
    "    # In practice, you'd likely use a conversation-aware prompting method.\n",
    "    combined_prompt = \"\\n\".join(msg[\"content\"] for msg in messages)\n",
    "    return get_completion(combined_prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "def rag_react_generate_mdd_documentation(file_path, model_metadata, sub_section_name, guidelines, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a Model Development Document (MDD) subsection report using a RAG-based ReAct prompting method.\n",
    "    \n",
    "    This function uses the RAG implementation in LatestHelp.py to load and chunk the input file,\n",
    "    builds a vector store, and then retrieves context from the stored documents. The retrieved context,\n",
    "    combined with the provided model metadata and subsection title, is used as the only knowledge base\n",
    "    when answering the policy questions.\n",
    "\n",
    "    Parameters:\n",
    "      - file_path (str): Path to the file (PDF, TXT, or CSV) containing the MDD information.\n",
    "      - model_metadata (str): Metadata describing the model.\n",
    "      - sub_section_name (str): Title of the MDD subsection.\n",
    "      - guidelines (list): A list of guideline questions/statements for the individual answers.\n",
    "      - model: The language model (e.g., a Hugging Face model).\n",
    "      - tokenizer: The tokenizer corresponding to the model.\n",
    "    \n",
    "    Returns:\n",
    "      - outputs (list): The list of individual guideline responses and the final comprehensive report.\n",
    "    \"\"\"\n",
    "    # Use the RAG implementation from LatestHelp.py to load documents and create a vector store.\n",
    "    vector_store = load_documents(file_path)\n",
    "    \n",
    "    # Combine all document chunks into one context string.\n",
    "    # (Alternatively, you could perform per-guideline retrieval, but here we combine for simplicity.)\n",
    "    context = \"\\n\".join(doc.content for doc in vector_store.documents)\n",
    "    \n",
    "    # Define the system prompt for individual responses.\n",
    "    system_prompt_individual_answers = (\n",
    "        \"Your task is to assist a Quantitative Model Validator working in the Model Risk Management team of a bank, to find answers to policy \"\n",
    "        \"questions about Model Development Document (MDD) based on the provided context. \\n\"\n",
    "        \"Contents of the subsection(s) of MDD is used as the only input context to answer the Model Validation policy questions. You are a \"\n",
    "        \"highly accurate assistant who strictly answers only based on the information in the provided context.\\n\\n\"\n",
    "        \"Strictly follow these Generation Instructions:\\n\"\n",
    "        \"- Your response should be accurate, coherent, detailed, and descriptive by including all the important statistics, tables, terminologies, and definitions.\\n\"\n",
    "        \"- Your response should be relevant to the question being asked.\\n\"\n",
    "        \"- Your response should be honest, focused, and grounded in the provided context.\\n\"\n",
    "        \"- Do not change or assume any definition, terminology, statistical data, numerical information, or table information.\\n\"\n",
    "        \"- Always respond with \\\"Not found\\\" when you cannot find relevant information in the context.\\n\"\n",
    "        \"- Always respond with \\\"Not found\\\" if any information asked is not explicitly mentioned.\\n\"\n",
    "        \"- Use the important keywords and phrases from the context to frame your response.\\n\"\n",
    "        \"- Use bullet points only when required.\\n\"\n",
    "        \"- Only use the information provided under the specific product, business segment or aspect when answering questions. If the context includes details about multiple products, ensure your response is limited to the product specified in the query. Do not include information from other products, businesses, or other aspects.\"\n",
    "    )\n",
    "    \n",
    "    # Define the main prompt that embeds the model metadata, subsection name, and the retrieved context.\n",
    "    prompt = f\"\"\"\n",
    "Consider model with metadata:\n",
    "{model_metadata}\n",
    "\n",
    "You are provided with long and detailed contents of subsection of MDD titled {sub_section_name} to be used as the only context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "DO NOT USE ANY OTHER INFORMATION OR YOUR OWN KNOWLEDGE APART FROM THE CONTEXT.\n",
    "Based on the above context information, provide a detailed, coherent and complete answer to the following question (treat a statement as a direct question and answer accordingly)\n",
    "\"\"\"\n",
    "    \n",
    "    # Initialize the conversation with the two system prompts.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_individual_answers},\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # Loop through each guideline to obtain individual responses.\n",
    "    for guideline in guidelines:\n",
    "        # Each guideline becomes a user prompt.\n",
    "        guideline_message = [{\"role\": \"user\", \"content\": guideline}]\n",
    "        output = generate_chat_template(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            messages=guideline_message,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=8000\n",
    "        )\n",
    "        messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "        \n",
    "        print(guideline)\n",
    "        print(\"#\" * 15)\n",
    "        print(output)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        outputs.append(output)\n",
    "    \n",
    "    # Build the final summarization prompt to merge all responses.\n",
    "    system_prompt_summarize_answers = f\"\"\"\n",
    "All the above answers to policy questions are required to create subsection titled {sub_section_name} for the Model Validation Document (MVD) report.\n",
    "Combine all the previous answers from the assistant in the same order and create a comprehensive and continuous report while following these instructions:\n",
    "- Final response should be continuous, accurate, coherent, detailed and descriptive\n",
    "- Final response should be grounded in the provided input context information, policy questions and their generated answers\n",
    "- Including all the statistics, tables, terminologies and definitions from the individual answers\n",
    "\"\"\"\n",
    "    messages = messages[:-2]  # Adjust conversation history if required.\n",
    "    messages.append({\"role\": \"user\", \"content\": system_prompt_summarize_answers})\n",
    "    \n",
    "    final_output = generate_chat_template(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=8000\n",
    "    )\n",
    "    messages.append({\"role\": \"assistant\", \"content\": final_output})\n",
    "    \n",
    "    print(\"#\" * 10)\n",
    "    print(final_output)\n",
    "    \n",
    "    outputs.append(final_output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters for the MDD subsection.\n",
    "    file_path = \"/commons/corpra_share/k152356/ReAct_Testing/metatext.txt\"\n",
    "    model_metadata = \"Example model metadata details, including version, type, and performance metrics.\"\n",
    "    sub_section_name = \"Business Portfolio Overview\"\n",
    "    guidelines = [\n",
    "        \"Briefly describe the business portfolio to which the model applies\",\n",
    "        \"Include the products and business segments offered by the business line\",\n",
    "        \"Describe the products of the LOB and portfolio to which this model applies.\",\n",
    "        \"Describe any current or planned changes in the products, channels, policies, programs, organization, or marketing practices that may impact the model under consideration.\",\n",
    "        \"Assess how close the current customer base to the target customer profile is.\",\n",
    "        \"Consider whether the customer base is likely to shift over the lifetime of the model.\",\n",
    "        \"Specify the current and possible future market conditions and the impact they may have on the portfolio and the model.\",\n",
    "        \"Describe the growth of the portfolio over the past 5, 10, X years, both in size and in significance to the balance sheet.\",\n",
    "        \"If this is a revalidation or the model replaces an existing model, highlight the key relevant changes on the business between the previous model developments and model validations, and this validation.\",\n",
    "        \"Include all the changes related to modeling, such as model framework and theory, variables, data sources, and programs; \"\n",
    "        \"business changes, such as policy or strategy; environmental changes, such as competitor actions, economic changes, and political or regulatory changes; \"\n",
    "        \"and any other changes that impact the model, its implementation, evaluation, and usage.\",\n",
    "        \"Include a table or summary of the portfolio, product, or business metrics of the business in the most recent and past periods. \"\n",
    "        \"These can include metrics such as balances, losses, recoveries, number of accounts, average account size, credit limits, etc.\"\n",
    "    ]\n",
    "    \n",
    "    # Assume model and tokenizer are already initialized as in LatestHelp.py.\n",
    "    model = None     # Replace with the actual model instance.\n",
    "    tokenizer = None # Replace with the actual tokenizer instance.\n",
    "    \n",
    "    outputs = rag_react_generate_mdd_documentation(file_path, model_metadata, sub_section_name, guidelines, model, tokenizer)\n",
    "    \n",
    "    print(\"Final generated outputs:\")\n",
    "    for idx, output in enumerate(outputs):\n",
    "        print(f\"Output {idx+1}:\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# System prompt for individual MDD answers\n",
    "# ------------------------------------------------------------------------------\n",
    "system_prompt_individual_answers = (\n",
    "    \"Your task is to assist a Quantitative Model Validator working in the Model Risk Management team of a bank, \"\n",
    "    \"to find answers to policy questions about Model Development Document (MDD) based on the provided context. \\n\"\n",
    "    \"Contents of the subsection(s) of MDD is used as the only input context to answer the Model Validation policy questions. You are a \"\n",
    "    \"highly accurate assistant who strictly answers only based on the information in the provided context.\\n\\n\"\n",
    "    \"Strictly follow these Generation Instructions:\\n\"\n",
    "    \"- Your response should be accurate, coherent, detailed, and descriptive by including all the important statistics, tables, terminologies, and definitions.\\n\"\n",
    "    \"- Your response should be relevant to the question being asked.\\n\"\n",
    "    \"- Your response should be honest, focused, and grounded in the provided context.\\n\"\n",
    "    \"- Do not change or assume any definition, terminology, statistical data, numerical information, or table information.\\n\"\n",
    "    \"- Always respond with \\\"Not found\\\" when you cannot find relevant information in the context.\\n\"\n",
    "    \"- Always respond with \\\"Not found\\\" if any information asked is not explicitly mentioned.\\n\"\n",
    "    \"- Use the important keywords and phrases from the context to frame your response.\\n\"\n",
    "    \"- Use bullet points only when required.\\n\"\n",
    "    \"- Only use the information provided under the specific product, business segment or aspect when answering questions. If the context includes details about multiple products, ensure your response is limited to the product specified in the query. Do not include information from other products, businesses, or other aspects.\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# List of guidelines for which to generate answers\n",
    "# ------------------------------------------------------------------------------\n",
    "guidelines = [\n",
    "    \"Briefly describe the business portfolio to which the model applies\",\n",
    "    \"Include the products and business segments offered by the business line\",\n",
    "    \"Describe the products of the LOB and portfolio to which this model applies.\",\n",
    "    \"Describe any current or planned changes in the products, channels, policies, programs, organization, or marketing practices that may impact the model under consideration.\",\n",
    "    \"Assess how close the current customer base to the target customer profile is.\",\n",
    "    \"Consider whether the customer base is likely to shift over the lifetime of the model.\",\n",
    "    \"Specify the current and possible future market conditions and the impact they may have on the portfolio and the model.\",\n",
    "    \"Describe the growth of the portfolio over the past 5, 10, X years, both in size and in significance to the balance sheet.\",\n",
    "    \"If this is a revalidation or the model replaces an existing model, highlight the key relevant changes on the business between the previous model developments and model validations, and this validation.\",\n",
    "    \"Include all the changes related to modeling, such as model framework and theory, variables, data sources, and programs; \"\n",
    "    \"business changes, such as policy or strategy; environmental changes, such as competitor actions, economic changes, and political or regulatory changes; \"\n",
    "    \"and any other changes that impact the model, its implementation, evaluation, and usage.\",\n",
    "    \"Include a table or summary of the portfolio, product, or business metrics of the business in the most recent and past periods. \"\n",
    "    \"These can include metrics such as balances, losses, recoveries, number of accounts, average account size, credit limits, etc.\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Function to answer guidelines using RAG with role declarations.\n",
    "# ------------------------------------------------------------------------------\n",
    "def answer_guidelines_rag_chat(context: str, guidelines: List[str], max_new_tokens: int = 512, temperature: float = 0.2) -> List[str]:\n",
    "    # Initialize conversation with system instructions and the context.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_individual_answers},\n",
    "        {\"role\": \"system\", \"content\": f\"Context:\\n{context}\"}\n",
    "    ]\n",
    "    outputs = []\n",
    "    \n",
    "    # Loop through each guideline.\n",
    "    for guideline in guidelines:\n",
    "        # Append the guideline as a user message.\n",
    "        messages.append({\"role\": \"user\", \"content\": guideline})\n",
    "        # Generate a response using the chat-based completion function.\n",
    "        response = get_completion_from_messages(messages, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        # Append the assistant's reply to the message history.\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        # Print the guideline and its answer.\n",
    "        print(\"Guideline:\")\n",
    "        print(guideline)\n",
    "        print(\"#\" * 30)\n",
    "        print(\"Answer:\")\n",
    "        print(response)\n",
    "        print(\"\\n\")\n",
    "        outputs.append(response)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main execution: Load documents, build vector store, and run guidelines using RAG\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and index your document file.\n",
    "    vector_store = load_documents(\"/commons/corpra_share/k152356/ReAct_Testing/metatext.txt\")\n",
    "    \n",
    "    # For each guideline, you could also retrieve context from the vector store. For demonstration,\n",
    "    # we'll assume you wish to use the full context from the file.\n",
    "    # You might retrieve context for a specific guideline as follows:\n",
    "    # results = vector_store.search(guideline, k=3)\n",
    "    # context = \"\\n\\n\".join([doc.content for doc, _ in results])\n",
    "    #\n",
    "    # Here, we assume 'context' is an aggregated extracted MDD text. You can retrieve and pass the specific context you desire.\n",
    "    aggregated_context = \"\\n\\n\".join([doc.content for doc in vector_store.documents])\n",
    "    \n",
    "    # Answer all guidelines using the chat-based role declaration approach.\n",
    "    guideline_answers = answer_guidelines_rag_chat(aggregated_context, guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Chat-based get_completion implementation with role declarations.\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_completion_from_messages(messages: List[Dict[str, str]], max_new_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "    # Combine the messages into a single prompt string with clear role markers.\n",
    "    prompt = \"\\n\".join(f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in messages)\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
