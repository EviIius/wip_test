{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIG: Modify to suit your environment/model as needed\n",
    "# ------------------------------------------------------------------------------\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "# Force a pad token to be eos if not already set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model for embeddings and for completion\n",
    "# (No 8-bit quantization here, just half-precision if GPU is available)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DATA CLASSES\n",
    "# ------------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    Simple wrapper for storing the chunk text and any associated metadata.\n",
    "    \"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT PROCESSOR\n",
    "# ------------------------------------------------------------------------------\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Helper class to load and clean files (PDF, TXT, CSV), then chunk them\n",
    "    into Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes extra whitespace, special chars, etc.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"\\.{2,}\", \"\", text)\n",
    "        text = re.sub(r\"\\s*\\u2002\\s*\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        lines = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if not line or all(c in \".-\" for c in line):\n",
    "                continue\n",
    "            if line.startswith(\"=== Page\"):\n",
    "                lines.append(line)\n",
    "                continue\n",
    "            if re.match(r\"^\\d+[-â€“]\\d+$\", line):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from each page of a PDF. (Requires PyPDF2)\n",
    "        \"\"\"\n",
    "        from PyPDF2 import PdfReader\n",
    "\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page_num, page in enumerate(reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text += f\"\\n=== Page {page_num} ===\\n{page_text}\"\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_txt(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Load raw text from a .txt file.\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_csv(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert CSV to a text blob (requires pandas).\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(file_path, dtype=str)  # read everything as string\n",
    "        text = df.to_string(index=False)\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text(\n",
    "        text: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Basic chunking logic using a custom 'split' approach or\n",
    "        you can leverage e.g. langchain's RecursiveCharacterTextSplitter.\n",
    "        \"\"\"\n",
    "\n",
    "        # Simple approach: split by ~chunk_size while overlapping\n",
    "        # You can make this more advanced with custom split strategies.\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        end = chunk_size\n",
    "\n",
    "        while start < len(text):\n",
    "            chunk = text[start:end]\n",
    "            if not chunk.strip():\n",
    "                break\n",
    "            doc = Document(content=chunk.strip())\n",
    "            chunks.append(doc)\n",
    "\n",
    "            # Overlap region\n",
    "            start = end - chunk_overlap\n",
    "            end = start + chunk_size\n",
    "\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def load_and_chunk_file(file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Wrapper to load a PDF, TXT, or CSV, clean the text, then chunk.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = DocumentProcessor.load_pdf(file_path)\n",
    "        elif ext == \".txt\":\n",
    "            text = DocumentProcessor.load_txt(file_path)\n",
    "        elif ext == \".csv\":\n",
    "            text = DocumentProcessor.load_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "        documents = DocumentProcessor.chunk_text(text)\n",
    "        return documents\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EMBEDDING & COMPLETION HELPERS\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Creates a single text embedding using the last hidden state of the\n",
    "    LLaMA model, normalized to unit length.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # last hidden layer\n",
    "        embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        embedding = embedding / embedding.norm(p=2)\n",
    "    return embedding.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Efficiently get embeddings in batches to avoid repeated overhead calls.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**batch_encodings, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            # We do a mean pooling for each item in the batch\n",
    "            for idx in range(len(batch)):\n",
    "                emb = hidden_states[idx].mean(dim=0)\n",
    "                emb = emb / emb.norm(p=2)\n",
    "                embeddings.append(emb.cpu().numpy().tolist())\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the LLaMA model with a basic sampling configuration.\n",
    "    \"\"\"\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,  # guard for input length\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# VECTOR STORE (FAISS)\n",
    "# ------------------------------------------------------------------------------\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Stores embeddings of Documents in a FAISS index and allows for\n",
    "    similarity search. \n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Attempts to load an existing FAISS index and documents list from disk.\n",
    "        Returns True if load is successful, else False.\n",
    "        \"\"\"\n",
    "        index_path = self._get_index_path()\n",
    "        docs_path = self._get_documents_path()\n",
    "        if os.path.exists(index_path) and os.path.exists(docs_path):\n",
    "            try:\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(docs_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded index with {len(self.documents)} documents.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Error loading index:\", e)\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        \"\"\"\n",
    "        Saves the FAISS index and documents list to disk.\n",
    "        \"\"\"\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error saving index:\", e)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        \"\"\"\n",
    "        Creates or loads the vector store index. If force_recreate is True,\n",
    "        a new index is built from the provided documents.\n",
    "        \"\"\"\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating a new FAISS index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "        embeddings = get_embeddings_batch(contents)\n",
    "\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Searches for the top-k documents relevant to the given query, returning\n",
    "        a list of (Document, similarity_score).\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "        \n",
    "        query_emb = get_embedding(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_emb]).astype(\"float32\"),\n",
    "            k\n",
    "        )\n",
    "\n",
    "        # Convert L2 distances to a simple similarity scale: similarity = 1 / (1 + distance)\n",
    "        similarities = 1 / (1 + distances)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            sim_score = similarities[0][i]\n",
    "            results.append((doc, sim_score))\n",
    "        return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# HYBRID SEARCHER (TF-IDF + VectorStore)\n",
    "# ------------------------------------------------------------------------------\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Example hybrid approach combining a TF-IDF search with a VectorStore search.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.vector_store = VectorStore(persist_directory)\n",
    "\n",
    "    def create_index(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        Creates a TF-IDF index and a VectorStore index over the same Documents.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self._initialize_tfidf()\n",
    "        self.vector_store.create_index(documents, force_recreate=True)\n",
    "\n",
    "    def _initialize_tfidf(self):\n",
    "        \"\"\"\n",
    "        Build the TF-IDF representation of all document content.\n",
    "        \"\"\"\n",
    "        contents = [doc.content for doc in self.documents]\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(contents)\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Combines vector store results and TF-IDF results, then returns top-k overall.\n",
    "        \"\"\"\n",
    "        # Vector results\n",
    "        vector_results = self.vector_store.search(query, k)\n",
    "\n",
    "        # TF-IDF results\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        keyword_indices = np.argsort(keyword_scores)[-k:][::-1]\n",
    "        keyword_results = [(self.documents[i], keyword_scores[i]) for i in keyword_indices]\n",
    "\n",
    "        # Combine\n",
    "        seen = set()\n",
    "        combined = []\n",
    "        for doc, score in (vector_results + keyword_results):\n",
    "            if doc.content not in seen:\n",
    "                seen.add(doc.content)\n",
    "                combined.append((doc, score))\n",
    "\n",
    "        # Sort by score desc, return top k\n",
    "        return sorted(combined, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant information based on the user_query,\n",
    "    then iterates over each guideline to generate segments of a final document.\n",
    "\n",
    "    This version includes metadata from each Document in the context block \n",
    "    for more precise references.\n",
    "\n",
    "    Returns a single combined result string.\n",
    "    \"\"\"\n",
    "    # 1) Pull top-k results from the vector store\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a consolidated context from the top results\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        metadata_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_strings = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            metadata_str = \"\\n\".join(meta_strings)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{metadata_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, create a \"segment\" for each\n",
    "    final_segments = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Focus on using the available context effectively.\n",
    "\"\"\"\n",
    "        segment_response = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_segment = f\"### GUIDELINE #{idx}: {guideline}\\n{segment_response.strip()}\\n\"\n",
    "        final_segments.append(formatted_segment)\n",
    "\n",
    "    # 4) Combine all segments into one final \"document\"\n",
    "    final_document = \"\\n\\n\".join(final_segments)\n",
    "    return final_document\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# USAGE EXAMPLE (Comment out or remove if not needed)\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load and chunk a sample file (PDF/TXT/CSV)\n",
    "    #    For demonstration, replace 'sample.pdf' with your own file path\n",
    "    #    or any .txt, .csv file. If PDF, make sure to install PyPDF2.\n",
    "    file_path = \"sample.pdf\"  # Change to your file path\n",
    "    doc_chunks = DocumentProcessor.load_and_chunk_file(file_path)\n",
    "\n",
    "    # 2) Create a VectorStore and index these chunks\n",
    "    vs = VectorStore(persist_directory=\"rag_index\")\n",
    "    vs.create_index(doc_chunks, force_recreate=True)\n",
    "\n",
    "    # 3) Example guidelines\n",
    "    my_guidelines = [\n",
    "        \"Provide a step-by-step approach.\",\n",
    "        \"Use non-technical language as much as possible.\",\n",
    "        \"Incorporate any numerical data precisely from the context.\"\n",
    "    ]\n",
    "\n",
    "    # 4) Generate a combined document addressing each guideline\n",
    "    user_query = \"How do I perform financial forecasting for a startup?\"\n",
    "    final_doc = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=my_guidelines,\n",
    "        vector_store=vs,\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== FINAL DOCUMENT =====\\n\")\n",
    "    print(final_doc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
