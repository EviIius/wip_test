{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant doc chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a single combined context from the top docs\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, generate a snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Use only the provided context if needed.\n",
    "\"\"\"\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from LatestHelp import VectorStore, Document, get_completion\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES (Separate Snippets)\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant doc chunks based on user_query,\n",
    "    then iterates over each guideline to produce a separate snippet.\n",
    "    Returns one combined \"document\" with all guideline-based responses.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k relevant documents\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a single combined context from the top docs\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        meta_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_parts = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            meta_str = \"\\n\".join(meta_parts)\n",
    "\n",
    "        context_str = (\n",
    "            f\"---\\nContent:\\n{doc.content}\\n\"\n",
    "            f\"Metadata:\\n{meta_str}\\n\"\n",
    "            f\"Similarity Score: {sim_score:.4f}\\n---\"\n",
    "        )\n",
    "        combined_context_parts.append(context_str)\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, generate a snippet\n",
    "    final_snippets = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Use only the provided context if needed.\n",
    "\"\"\"\n",
    "        snippet = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_snippet = f\"### GUIDELINE #{idx}: {guideline}\\n{snippet.strip()}\\n\"\n",
    "        final_snippets.append(formatted_snippet)\n",
    "\n",
    "    # 4) Combine all guideline-based snippets\n",
    "    final_document = \"\\n\\n\".join(final_snippets)\n",
    "    return final_document\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ALTERNATIVE: SINGLE CONSOLIDATED REPORT\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_consolidated_report(\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    user_query: str = \"\",\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Loops over each guideline, retrieves relevant context, and\n",
    "    builds ONE single consolidated report that addresses them all.\n",
    "    Optionally includes the 'user_query' to provide more context\n",
    "    for the final prompt if desired.\n",
    "    \"\"\"\n",
    "    # Step 1: Create a dictionary to collect top retrieved references for each guideline\n",
    "    all_retrieved = {}\n",
    "\n",
    "    # Step 2: For each guideline, retrieve the top-k relevant doc chunks\n",
    "    for guideline in guidelines:\n",
    "        search_results = vector_store.search(guideline, k=k)  # [(Document, score), ...]\n",
    "        \n",
    "        # Gather doc contents\n",
    "        relevant_passages = [doc.content for doc, _ in search_results]\n",
    "        all_retrieved[guideline] = relevant_passages\n",
    "\n",
    "    # Step 3: Build a single prompt that includes ALL guidelines and retrieved context\n",
    "    final_prompt = \"You are an expert tasked with creating ONE cohesive report.\\n\"\n",
    "    if user_query:\n",
    "        final_prompt += f\"User Query: {user_query}\\n\"\n",
    "    final_prompt += (\n",
    "        \"\\nBelow are multiple guidelines, each followed by top retrieved passages.\\n\"\n",
    "        \"Use them to synthesize one final, cohesive document:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for i, guideline in enumerate(guidelines, start=1):\n",
    "        final_prompt += f\"GUIDELINE {i}: {guideline}\\n\"\n",
    "        final_prompt += \"Top Matching Passages:\\n\"\n",
    "        for j, passage in enumerate(all_retrieved[guideline], start=1):\n",
    "            # Optionally truncate or segment the passage\n",
    "            final_prompt += f\"  [{j}] {passage[:500]}...\\n\"\n",
    "        final_prompt += \"\\n\"\n",
    "\n",
    "    final_prompt += (\n",
    "        \"Please produce a SINGLE consolidated document that addresses all guidelines. \"\n",
    "        \"Focus on clarity, cohesion, and completeness.\\n\\n\"\n",
    "        \"Begin your final report now:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Generate one final completion\n",
    "    final_report = get_completion(\n",
    "        prompt=final_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Example Usage\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example guidelines\n",
    "    guidelines_list = [\n",
    "        \"Guideline 1: Handling user data under GDPR\",\n",
    "        \"Guideline 2: Best practices for data retention\",\n",
    "        \"Guideline 3: Encryption requirements at rest and in transit\",\n",
    "    ]\n",
    "\n",
    "    # Example user query\n",
    "    user_query = \"What do we need to do to ensure compliance with user data regulations?\"\n",
    "\n",
    "    # Initialize or load a VectorStore\n",
    "    store = VectorStore(persist_directory=\"rag_index\")\n",
    "    loaded = store.load_local_index()\n",
    "    if not loaded:\n",
    "        # If the index doesn't exist, you'd create it from documents:\n",
    "        # docs = [Document(...), Document(...), ...]\n",
    "        # store.create_index(docs, force_recreate=True)\n",
    "        pass\n",
    "\n",
    "    # Example 1: Generate separate guideline-based snippets\n",
    "    final_doc_snippets = generate_document_with_guidelines(\n",
    "        user_query, \n",
    "        guidelines_list, \n",
    "        vector_store=store,\n",
    "        k=3, \n",
    "        max_new_tokens=512, \n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"=== SEPARATE SNIPPETS ===\")\n",
    "    print(final_doc_snippets)\n",
    "\n",
    "    # Example 2: Generate one consolidated final report\n",
    "    consolidated_report = generate_consolidated_report(\n",
    "        guidelines=guidelines_list,\n",
    "        vector_store=store,\n",
    "        user_query=user_query,\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"\\n=== SINGLE CONSOLIDATED REPORT ===\")\n",
    "    print(consolidated_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# EXAMPLE USAGE (Comment out if you only want library code)\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Suppose we load & chunk a sample file (PDF, TXT, or CSV)\n",
    "    file_path = \"sample.txt\"  # adjust to your actual file\n",
    "    doc_chunks = DocumentProcessor.load_and_chunk_file(file_path)\n",
    "\n",
    "    # 2) Create a vector store and index the doc chunks\n",
    "    vs = VectorStore(persist_directory=\"rag_index\")\n",
    "    vs.create_index(doc_chunks, force_recreate=True)\n",
    "\n",
    "    # 3) Some example guidelines\n",
    "    my_guidelines = [\n",
    "        \"Provide a step-by-step approach.\",\n",
    "        \"Use plain language for a broad audience.\"\n",
    "    ]\n",
    "\n",
    "    # 4) Generate a combined doc addressing each guideline\n",
    "    user_query = \"How can I plan my personal finances effectively?\"\n",
    "    final_doc = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=my_guidelines,\n",
    "        vector_store=vs,\n",
    "        k=3,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== FINAL DOCUMENT =====\\n\")\n",
    "    print(final_doc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
