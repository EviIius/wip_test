{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIG: Modify to suit your environment/model as needed\n",
    "# ------------------------------------------------------------------------------\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model for embeddings and completion\n",
    "# (No 8-bit quantization here, just standard torch float16 for GPU)\n",
    "embedding_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DATA CLASSES\n",
    "# ------------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Document:\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EMBEDDING & COMPLETION HELPERS\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Creates a single text embedding using the last hidden state\n",
    "    of your LLaMA model, normalized to unit length.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # last hidden layer\n",
    "        embedding = hidden_states.mean(dim=1).squeeze()\n",
    "        embedding = embedding / embedding.norm(p=2)\n",
    "    return embedding.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Efficiently get embeddings in batches to avoid repeated overhead calls.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**batch_encodings, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]  # last hidden layer\n",
    "            # We do a mean pooling for each item in the batch:\n",
    "            for idx in range(len(batch)):\n",
    "                emb = hidden_states[idx].mean(dim=0)\n",
    "                emb = emb / emb.norm(p=2)\n",
    "                embeddings.append(emb.cpu().numpy().tolist())\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the LLaMA model with a basic sampling configuration.\n",
    "    \"\"\"\n",
    "    input_data = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024,  # guard for input length\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = completion_model.generate(\n",
    "            **input_data,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# VECTOR STORE (FAISS)\n",
    "# ------------------------------------------------------------------------------\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Stores embeddings of Documents in a FAISS index and allows for\n",
    "    similarity search.\n",
    "    \"\"\"\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Attempts to load an existing FAISS index and documents list from disk.\n",
    "        Returns True if load is successful, else False.\n",
    "        \"\"\"\n",
    "        index_path = self._get_index_path()\n",
    "        docs_path = self._get_documents_path()\n",
    "        if os.path.exists(index_path) and os.path.exists(docs_path):\n",
    "            try:\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(docs_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded index with {len(self.documents)} documents.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Error loading index:\", e)\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        \"\"\"\n",
    "        Saves the FAISS index and documents list to disk.\n",
    "        \"\"\"\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error saving index:\", e)\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        \"\"\"\n",
    "        Creates or loads the vector store index. If force_recreate is True,\n",
    "        a new index is built from the provided documents.\n",
    "        \"\"\"\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating a new FAISS index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "        embeddings = get_embeddings_batch(contents)\n",
    "\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Searches for the top-k documents relevant to the given query, returning\n",
    "        a list of (Document, similarity_score).\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "        \n",
    "        query_emb = get_embedding(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_emb]).astype(\"float32\"),\n",
    "            k\n",
    "        )\n",
    "\n",
    "        # Convert L2 distances to a simple similarity scale: similarity = 1 / (1 + distance)\n",
    "        similarities = 1 / (1 + distances)\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            sim_score = similarities[0][i]\n",
    "            results.append((doc, sim_score))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DOCUMENT GENERATION WITH GUIDELINES\n",
    "# ------------------------------------------------------------------------------\n",
    "def generate_document_with_guidelines(\n",
    "    user_query: str,\n",
    "    guidelines: List[str],\n",
    "    vector_store: VectorStore,\n",
    "    k: int = 3,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Searches the VectorStore for relevant information based on the user_query,\n",
    "    then iterates over each guideline to generate segments of a final document.\n",
    "\n",
    "    This version includes metadata from each Document in the context block \n",
    "    for more precise references.\n",
    "\n",
    "    Returns a single combined result string.\n",
    "    \"\"\"\n",
    "    # 1) Pull top-k results from the vector store\n",
    "    search_results = vector_store.search(user_query, k=k)\n",
    "\n",
    "    # 2) Build a consolidated context from the top results, \n",
    "    #    including metadata for clarity\n",
    "    combined_context_parts = []\n",
    "    for doc, sim_score in search_results:\n",
    "        # Example usage of metadata; adapt as needed if you store page numbers, titles, etc.\n",
    "        metadata_str = \"\"\n",
    "        if doc.metadata:\n",
    "            meta_strings = [f\"{key}: {val}\" for key, val in doc.metadata.items()]\n",
    "            metadata_str = \"\\n\".join(meta_strings)\n",
    "\n",
    "        context_str = f\"---\\nContent:\\n{doc.content}\\nMetadata:\\n{metadata_str}\\nSimilarity Score: {sim_score:.4f}\\n---\"\n",
    "        combined_context_parts.append(context_str)\n",
    "\n",
    "    combined_context = \"\\n\\n\".join(combined_context_parts)\n",
    "\n",
    "    # 3) Loop over each guideline, create a \"segment\" for each guideline\n",
    "    #    using the context. We accumulate them in one final output.\n",
    "    final_segments = []\n",
    "    for idx, guideline in enumerate(guidelines, start=1):\n",
    "        # Construct a prompt that includes:\n",
    "        #   - The user query\n",
    "        #   - The relevant context from all top documents\n",
    "        #   - The specific guideline\n",
    "        prompt = f\"\"\"\n",
    "You have the following user query:\n",
    "{user_query}\n",
    "\n",
    "Context from relevant documents (with metadata):\n",
    "{combined_context}\n",
    "\n",
    "Guideline #{idx}: {guideline}\n",
    "\n",
    "Based on the user query and the above context, create a concise \n",
    "section of a final document that follows this guideline. \n",
    "Focus on using the available context effectively.\n",
    "\"\"\"\n",
    "        segment_response = get_completion(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        formatted_segment = f\"### GUIDELINE #{idx}: {guideline}\\n{segment_response.strip()}\\n\"\n",
    "        final_segments.append(formatted_segment)\n",
    "\n",
    "    # 4) Combine all segments into one final \"document\"\n",
    "    final_document = \"\\n\\n\".join(final_segments)\n",
    "    return final_document\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# USAGE EXAMPLE (comment out in production if not needed)\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    store = VectorStore(persist_directory=\"my_rag_index\")\n",
    "    # Assume we've already created the index with store.create_index(...) previously\n",
    "\n",
    "    # Some dummy guidelines:\n",
    "    my_guidelines = [\n",
    "        \"Provide a step-by-step approach.\",\n",
    "        \"Make the language accessible to non-technical readers.\",\n",
    "        \"Incorporate any numerical data or references precisely.\"\n",
    "    ]\n",
    "\n",
    "    user_query = \"How do I set up financial forecasting for my new startup?\"\n",
    "    result_doc = generate_document_with_guidelines(\n",
    "        user_query=user_query,\n",
    "        guidelines=my_guidelines,\n",
    "        vector_store=store,\n",
    "        k=3\n",
    "    )\n",
    "    print(\"\\n----- FINAL DOCUMENT -----\\n\", result_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
