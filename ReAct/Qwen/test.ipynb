{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Import Transformers for Qwen2\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c30ff4927448ca8d9c08107288e917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Choose the model variant\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Switch to Float32 for better compatibility\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # Adjust this based on your GPU memory\n",
    "num_workers = 8 # Number of CPU workers for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for batching text inputs\"\"\"\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embeddings from LLaMA 2\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Take the mean of the last hidden state as the embedding\n",
    "    embedding = outputs.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy().tolist()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], batch_size: int = 50) -> List[List[float]]:\n",
    "    \"\"\"Get embeddings for multiple texts with progress bar\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with tqdm(total=len(texts), desc=\"Generating embeddings\") as pbar:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = [get_embedding(text) for text in batch]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            pbar.update(len(batch))\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_parallel(texts, batch_size=64, num_workers=4):\n",
    "    \"\"\"Parallelized embedding generation using DataLoader and FP16\"\"\"\n",
    "    dataset = TextDataset(texts)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        with tqdm(total=len(texts), desc=\"Generating embeddings\") as pbar:\n",
    "            for batch in loader:\n",
    "                # Tokenize in batch for faster processing\n",
    "                inputs = tokenizer(list(batch), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Extract embeddings from the last hidden state\n",
    "                batch_embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy().tolist()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_completion(\n",
    "    messages: str,\n",
    "    max_length: int = 200,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Get completion from LLaMA 2\"\"\"\n",
    "    input_ids = tokenizer(messages, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return completion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Class\n",
    "@dataclass\n",
    "class Document:\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "# Document Processor\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = re.sub(r\"\\.{2,}\", \"\", text)\n",
    "        text = re.sub(r\"\\s*\\u2002\\s*\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        lines = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if not line or all(c in \".-\" for c in line):\n",
    "                continue\n",
    "            if line.startswith(\"=== Page\"):\n",
    "                lines.append(line)\n",
    "                continue\n",
    "            if re.match(r\"^\\d+[-–]\\d+$\", line):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            total_pages = len(reader.pages)\n",
    "\n",
    "            with tqdm(total=total_pages, desc=\"Loading PDF pages\") as pbar:\n",
    "                for page_num, page in enumerate(reader.pages, 1):\n",
    "                    page_text = page.extract_text()\n",
    "                    text += f\"\\n=== Page {page_num} ===\\n{page_text}\"\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(\"Cleaning extracted text...\")\n",
    "        return DocumentProcessor.clean_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text(\n",
    "        text: str, chunk_size: int = 1000, chunk_overlap: int = 200\n",
    "    ) -> List[Document]:\n",
    "        separators = [\"\\n=== Page\", \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "\n",
    "        chunks = splitter.split_text(text)\n",
    "        processed_chunks = []\n",
    "\n",
    "        with tqdm(chunks, desc=\"Processing chunks\", unit=\"chunk\") as pbar:\n",
    "            for chunk in pbar:\n",
    "                chunk = DocumentProcessor.clean_text(chunk)\n",
    "                if not chunk.startswith(\"=== Page\"):\n",
    "                    sentence_boundaries = [\". \", \"? \", \"! \"]\n",
    "                    first_boundary = float(\"inf\")\n",
    "                    for boundary in sentence_boundaries:\n",
    "                        pos = chunk.find(boundary)\n",
    "                        if pos != -1 and pos < first_boundary:\n",
    "                            first_boundary = pos\n",
    "                    if first_boundary < 100 and first_boundary != float(\"inf\"):\n",
    "                        chunk = chunk[first_boundary + 2 :]\n",
    "\n",
    "                if chunk.strip():\n",
    "                    processed_chunks.append(Document(content=chunk))\n",
    "                pbar.set_postfix({\"total_chunks\": len(processed_chunks)})\n",
    "\n",
    "        print(f\"Final number of chunks: {len(processed_chunks)}\")\n",
    "        return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, persist_directory: str = \"rag_index\"):\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.persist_directory = persist_directory\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    def _get_index_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"faiss.index\")\n",
    "\n",
    "    def _get_documents_path(self) -> str:\n",
    "        return os.path.join(self.persist_directory, \"documents.pkl\")\n",
    "\n",
    "    def load_local_index(self) -> bool:\n",
    "        index_path = self._get_index_path()\n",
    "        documents_path = self._get_documents_path()\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(index_path) and os.path.exists(documents_path):\n",
    "                self.index = faiss.read_index(index_path)\n",
    "                with open(documents_path, \"rb\") as f:\n",
    "                    self.documents = pickle.load(f)\n",
    "                print(f\"Loaded existing index with {len(self.documents)} documents\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index: {e}\")\n",
    "            self.index = None\n",
    "            self.documents = []\n",
    "\n",
    "        return False\n",
    "\n",
    "    def save_local_index(self):\n",
    "        if self.index is None or not self.documents:\n",
    "            return\n",
    "        try:\n",
    "            faiss.write_index(self.index, self._get_index_path())\n",
    "            with open(self._get_documents_path(), \"wb\") as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "            print(f\"Saved index with {len(self.documents)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving index: {e}\")\n",
    "\n",
    "    def create_index(self, documents: List[Document], force_recreate: bool = False):\n",
    "        if not force_recreate and self.load_local_index():\n",
    "            return\n",
    "\n",
    "        print(\"Creating new index...\")\n",
    "        self.documents = documents\n",
    "        contents = [doc.content for doc in documents]\n",
    "        embeddings = get_embeddings_batch(contents)\n",
    "\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        # Use IndexFlatL2 instead of IndexFlatIP\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "        self.save_local_index()\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not initialized. Call create_index first.\")\n",
    "\n",
    "        query_embedding = get_embedding(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding]).astype(\"float32\"), k\n",
    "        )\n",
    "\n",
    "        # Convert distances to similarity scores\n",
    "        similarities = 1 / (1 + distances)\n",
    "\n",
    "        return [\n",
    "            (self.documents[i], similarities[0][idx])\n",
    "            for idx, i in enumerate(indices[0])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b596c550224aa1addfacf4dffe5d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading PDF pages:   0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning extracted text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43ab726ee9444a2b047d4dd73e2b147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/2074 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of chunks: 2074\n",
      "Creating new index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa851d0f920d427fa2b065886119a9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index with 2074 documents\n",
      "Processed 2074 chunks\n"
     ]
    }
   ],
   "source": [
    "def load_documents(file_path):\n",
    "    # Load and chunk document\n",
    "    text = DocumentProcessor.load_pdf(file_path)\n",
    "    documents = DocumentProcessor.chunk_text(text)\n",
    "    \n",
    "    # Create and return vector store\n",
    "    vector_store = VectorStore()  # HybridSearcher()\n",
    "    vector_store.create_index(documents, force_recreate=True)\n",
    "    print(f\"Processed {len(documents)} chunks\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "vector_store = load_documents(\"WF_benefits_book.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing coverage What changes can you make during the year\n",
      "Address to submit an order or notice A Medical Child Support Order or National Medical Support Notice should be submitted to: Wells Fargo MAC: N9160-014 DCCOE Ben Admin 1801 Park View Dr. 1st Floor Shoreview, MN 55126-5030 Determining if the order or notice is qualified The plan administrator delegates Wells Fargo HR Operations Benefits Administration to receive and process the orders\n",
      "For more information, see the “Orthodontia claims” section on page 3-10 . Chapter 3: Dental Plan 3-8 === Page 197 === What is not covered Charges for some types of dental work will not be covered by the dental plan\n",
      "Agent for service of legal process The agent for service of legal process is ARAG. All correspondence should be directed to ARAG at: ARAG Attn: Legal Department 500 Grand Ave\n",
      "Unless otherwise indicated, references i n this chapter to the HSA Plan and Copay Plan with HRA include the applicable Out of Area coverage. • For interns and flexible employees: – Flexible High Deductible Health Plan (Flex HDHP) Note: The health savings account that you open in conjunction with your enrollment in the HSA Plan is not part of the Health Plan or any other ERISA-covered benefit plan sponsored by Wells Fargo\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 376, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     answer \u001b[38;5;241m=\u001b[39m get_completion(prompt)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat benefits are listed by Wells Fargo?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36mquery\u001b[1;34m(query, top_k)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(contents)\n\u001b[0;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m You are a helpful assistant.\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mYou have to answer the contents below:\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontents\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m, in \u001b[0;36mget_completion\u001b[1;34m(messages, max_length, temperature)\u001b[0m\n\u001b[0;32m     50\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(messages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 53\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m completion \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32mc:\\Users\\rbrul\\Documents\\GitHub\\wip_test\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rbrul\\Documents\\GitHub\\wip_test\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2108\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   2106\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_params\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rbrul\\Documents\\GitHub\\wip_test\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1411\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1410\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1415\u001b[0m     )\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1421\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 376, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "def query(query, top_k=5):\n",
    "    contents = vector_store.search(query, k=top_k)\n",
    "    contents = '\\n'.join([x[0].content for x in contents])\n",
    "    print(contents)\n",
    "    \n",
    "    prompt = f\"\"\" You are a helpful assistant.\n",
    "    You have to answer the contents below:\n",
    "    Context: {contents}\n",
    "    Query: {query}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    answer = get_completion(prompt)\n",
    "    print(answer)\n",
    "\n",
    "query(\"What benefits are listed by Wells Fargo?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
