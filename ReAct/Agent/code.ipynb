{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42af7c0d9fb24b61bd82e81a16fd8c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
      "     ---------------------------------------- 0.0/66.7 MB ? eta -:--:--\n",
      "     ------ -------------------------------- 10.5/66.7 MB 65.5 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 33.3/66.7 MB 92.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 55.6/66.7 MB 98.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 66.7/66.7 MB 88.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-cpp-python) (2.2.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp312-cp312-win_amd64.whl size=3795411 sha256=e54ec73ae084327fd1a2c3d2a0717a2b83adf30b0e33dd8a773c175f6537f071\n",
      "  Stored in directory: c:\\users\\rbrul\\appdata\\local\\pip\\cache\\wheels\\c9\\b1\\23\\8a682c248add4288df3d136a788adcea3df7fdac1bca426799\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade accelerate\n",
    "# !pip show transformers accelerate bitsandbytes\n",
    "# !pip install --upgrade transformers accelerate bitsandbytes\n",
    "# !pip uninstall accelerate -y\n",
    "# !pip install accelerate --no-cache-dir\n",
    "# !pip install llama-index\n",
    "!pip install llama-cpp-python --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from IPython.display import display_markdown\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f76fd9cf4f54ad3b18397b7837a87d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\", torch.cuda.current_device())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normal Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_chat_template(model, tokenizer, sys_pmt=\"\", prompt=\"\", temperature=0.7, top_p=0.9, max_new_tokens=1000, do_sample=True):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_pmt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    pipeline_ = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Ensure EOS and PAD tokens exist\n",
    "    eos_token_id = tokenizer.eos_token_id or tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "    pad_token_id = tokenizer.pad_token_id or eos_token_id  # Use EOS as a fallback\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    outputs = pipeline_(\n",
    "        prompt, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=eos_token_id, \n",
    "        pad_token_id=pad_token_id,  # Explicitly set pad token\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    \n",
    "    return outputs[0][\"generated_text\"].split(prompt)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a paraphrased version of the text:\n",
       "\n",
       "Our enterprise support platform, Central Analytics Support & Enablement (CASE), is designed for ~20k data scientists, analysts, engineers, and support teams. CASE features a range of capabilities, including a unique chatbot, \"CASEY,\" which utilizes intent-driven technology and is built on Microsoft Copilot SDKs. The platform also incorporates the Collective Intelligence framework, a conditional ticketing system, and MS Graph-enabled search functionality. These features allow product and support teams to effectively meet customer needs, manage product documentation, and address product-related issues efficiently."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paraphrasing Prompt Basic Example\n",
    "system_prompt = \"\"\"\n",
    "You are a basic paraphrasing tool.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Paraphrase the following text:\n",
    "\n",
    "Central Analytics Support & Enablement (CASE) is our enterprise support platform for the ~20k data scientists, data analysts, MLOps engineers, BI developers, and other support teams. CASE includes multiple features such as \"CASEY\" the chatbot, an intent-driven (i.e. non-Gen AI/non-LLM) chatbot microservice built on MS Copilot SDKs. Additionally, CASE includes the Collective Intelligence (CI) framework, conditional ticketing system, and MS Graph-enabled search. All of this enables product and support teams to efficiently address customer needs, manage their product knowledge documentation, and handle product-related issues.\n",
    "\"\"\"\n",
    "\n",
    "output = generate_chat_template(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sys_pmt=system_prompt,\n",
    "    prompt=prompt,\n",
    "    temperature=0.7,  # Allows for more variation\n",
    "    top_p=0.9, # Nucleus sampling\n",
    "    do_sample=True    # Enables sampling\n",
    ")\n",
    "display_markdown(output, raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ReAct Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (0.12.17)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.12.17)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.6.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.3.19)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.62.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2025.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
      "Requirement already satisfied: anyio in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface\n",
      "  Downloading llama_index_llms_huggingface-0.4.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-llms-huggingface) (0.28.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-llms-huggingface) (0.12.17)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-llms-huggingface) (2.6.0+cu126)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.48.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.17.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.5.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (6.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->llama-index-llms-huggingface) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface) (1.3.1)\n",
      "Downloading llama_index_llms_huggingface-0.4.2-py3-none-any.whl (11 kB)\n",
      "Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
      "Successfully installed llama-index-llms-huggingface-0.4.2 text-generation-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.1-py3-none-any.whl.metadata (767 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.28.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.12.17)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.12)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.38)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.17.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.48.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0+cu126)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.1-py3-none-any.whl (8.9 kB)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  11.0/11.1 MB 68.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 49.5 MB/s eta 0:00:00\n",
      "Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn, sentence-transformers, llama-index-embeddings-huggingface\n",
      "Successfully installed llama-index-embeddings-huggingface-0.5.1 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-huggingface-api\n",
      "  Downloading llama_index_embeddings_huggingface_api-0.3.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (0.28.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-embeddings-huggingface-api) (0.12.17)\n",
      "Collecting llama-index-utils-huggingface<0.4.0,>=0.3.0 (from llama-index-embeddings-huggingface-api)\n",
      "  Downloading llama_index_utils_huggingface-0.3.0-py3-none-any.whl.metadata (697 bytes)\n",
      "Requirement already satisfied: filelock in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (3.11.12)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (2.0.38)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface-api) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rbrul\\documents\\github\\wip_test\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface-api) (1.3.1)\n",
      "Downloading llama_index_embeddings_huggingface_api-0.3.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_utils_huggingface-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Installing collected packages: llama-index-utils-huggingface, llama-index-embeddings-huggingface-api\n",
      "Successfully installed llama-index-embeddings-huggingface-api-0.3.0 llama-index-utils-huggingface-0.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-embeddings-huggingface-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     token=hf_token,\n",
    "# )\n",
    "\n",
    "# stopping_ids = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# # Optional quantization to 4bit\n",
    "# # import torch\n",
    "# # from transformers import BitsAndBytesConfig\n",
    "\n",
    "# # quantization_config = BitsAndBytesConfig(\n",
    "# #     load_in_4bit=True,\n",
    "# #     bnb_4bit_compute_dtype=torch.float16,\n",
    "# #     bnb_4bit_quant_type=\"nf4\",\n",
    "# #     bnb_4bit_use_double_quant=True,\n",
    "# # )\n",
    "\n",
    "# llm = HuggingFaceLLM(\n",
    "#     model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     model_kwargs={\n",
    "#         \"token\": hf_token,\n",
    "#         \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "#         # \"quantization_config\": quantization_config\n",
    "#     },\n",
    "#     generate_kwargs={\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.6,\n",
    "#         \"top_p\": 0.9,\n",
    "#     },\n",
    "#     tokenizer_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     tokenizer_kwargs={\"token\": hf_token},\n",
    "#     stopping_ids=stopping_ids,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd17d70640b8409bbf159fc6aefc9d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is successfully loaded on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "# Load Model Manually\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\",  # Auto-detect GPU if available\n",
    ")\n",
    "\n",
    "# Define Stopping IDs\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "# Pass Manually Loaded Model to HuggingFaceLLM\n",
    "llm = HuggingFaceLLM(\n",
    "    model=model,  # Use preloaded model\n",
    "    tokenizer=tokenizer,  # Use preloaded tokenizer\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    stopping_ids=stopping_ids,\n",
    ")\n",
    "\n",
    "print(\"Model is successfully loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham is a British-American computer scientist, entrepreneur, and venture capitalist. He is the co-founder of Y Combinator, a well-known startup accelerator, and has been a prominent figure in the tech industry for over two decades.\n",
      "Paul Graham was born on November 4, 1964, in London, England. He grew up in a family of modest means and developed an interest in computers at an early age. Graham earned a Bachelor of Arts degree in mathematics from the University of Illinois and later earned a Master of Science degree in computer science from the University of California, Berkeley.\n",
      "Graham began his career as a programmer and later co-founded several companies, including Viaweb, a company that developed an online shopping cart for Amazon. In 1998, Viaweb was acquired by Yahoo! for $49 million. Graham used the funds from the acquisition to start Y Combinator, which has since become one of the most successful startup accelerators in the world.\n",
      "Y Combinator has invested in over 2,000 startups, including Dropbox, Airbnb, Reddit, and Stripe, among many others. Graham has also written extensively on startup culture and the tech industry, and has been a vocal advocate for entrepreneurship and innovation.\n",
      "Graham has been recognized for his\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who is Paul Graham?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" \"paul_graham_essay.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Llama-3.1-8B-Instruct model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did paul graham do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divides two integers and returns the result integer\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "divide_tool = FunctionTool.from_defaults(fn=divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    [multiply_tool, add_tool, subtract_tool, divide_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.chat(\"What is (121 + 2) * 5?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
